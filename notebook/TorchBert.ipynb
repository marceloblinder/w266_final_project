{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 972
    },
    "colab_type": "code",
    "id": "lA0BbXCIphJn",
    "outputId": "f43fddec-ebd2-49ab-8371-95ba80918ee8"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "from transformers.modeling_bert import BertEmbeddings, BertLayerNorm, BertModel, BertPreTrainedModel, gelu\n",
    "from transformers import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    filename='bert.log')\n",
    "logger = logging.getLogger('bert.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FN0GUySc2zxf",
    "outputId": "864a3b2a-0a26-490e-b7f1-384bd0b2c962"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to load/save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QT2HeFPduMzG"
   },
   "outputs": [],
   "source": [
    "root_folder = '../bert'\n",
    "\n",
    "def save_model(model, model_filename, folder=root_folder):\n",
    "    ''' Save a pytorch model '''\n",
    "    model_file = os.path.join(folder, model_filename)\n",
    "\n",
    "    if os.path.exists(model_file):\n",
    "        os.remove(model_file)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "\n",
    "def load_model(folder, version):\n",
    "    ''' Load a pytorch model '''\n",
    "    config_filename = f'config_{version}.pkl'  \n",
    "\n",
    "    with open(os.path.join(folder, config_filename), 'rb') as f:\n",
    "        config = pickle.load(f)\n",
    "\n",
    "    config.output_hidden_states = True\n",
    "    model = BertForMultiLabelSequenceClassification(config)\n",
    "\n",
    "    file = os.path.join(folder, f'bert_{version}_cuda.pt')\n",
    "    state = torch.load(file) if is_cuda else torch.load(file, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    if is_cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    return model\n",
    "\n",
    "def remove_last_model(folder, model_filename, config_filename):\n",
    "    model_file = os.path.join(folder, model_filename)\n",
    "\n",
    "    if os.path.exists(model_file):\n",
    "        os.remove(model_file)\n",
    "\n",
    "    config_file = os.path.join(folder, config_filename)\n",
    "\n",
    "    if os.path.exists(config_file):\n",
    "        os.remove(config_file)\n",
    "\n",
    "def file_exists(filename, folder=root_folder):\n",
    "    return os.path.exists(os.path.join(folder, filename))\n",
    "\n",
    "def save_object(filename, obj, folder=root_folder):\n",
    "    obj_file = os.path.join(folder, filename)\n",
    "\n",
    "    if os.path.exists(obj_file):\n",
    "        os.remove(obj_file)\n",
    "\n",
    "    with open(obj_file, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_object(filename, folder=root_folder):\n",
    "    object_file = os.path.join(folder, filename)\n",
    "\n",
    "    with open(object_file, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "z3fJqCe-puL0",
    "outputId": "4a25ad99-56da-4a04-be00-46d0f29b974e"
   },
   "outputs": [],
   "source": [
    "model_class = BertModel\n",
    "tokenizer_class = BertTokenizer\n",
    "pretrained_weights = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EiiqXq9M8ieC"
   },
   "outputs": [],
   "source": [
    "class BertForMultiLabelSequenceClassification(BertPreTrainedModel):\n",
    "    \"\"\"BERT model for multiple label classification.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        num_labels = len(label_columns)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob) # https://arxiv.org/abs/1207.0580\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        self.loss_fct = BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        hidden, pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None: \n",
    "            # Training the model                \n",
    "            return self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "        \n",
    "        else:\n",
    "            return hidden, pooled_output, logits\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input (text) processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPPFrVtV996R"
   },
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.labels = labels\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUKr9OU59-PE"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    features = []\n",
    "    count = 0\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    for example in examples:\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "        \n",
    "        labels_ids = [float(label) for label in example.labels]\n",
    "        features.append(InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=labels_ids))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GZjUNwB3aRl"
   },
   "outputs": [],
   "source": [
    "# Load the full dataset into a DataFrame\n",
    "df = pd.read_parquet('../data/nyt_full.parquet.gz')\n",
    "SEED = 17\n",
    "train, test = train_test_split(df, test_size=.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Uq4Q6rD_3lJV",
    "outputId": "e4be128b-2ebc-4952-8230-34cd07d46fc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630802\n",
      "157701\n"
     ]
    }
   ],
   "source": [
    "print(len(train.Id))\n",
    "print(len(test.Id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZG2AW2naOOmT",
    "outputId": "d0ec60cc-3e8b-4fe0-cd17-e2b7d3fc7258"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 80\n",
    "label_columns = [c for c in train.columns if c not in ['Id', 'Text']]\n",
    "\n",
    "def get_labels(data, i):\n",
    "    return data.iloc[i][label_columns]\n",
    "\n",
    "def create_features(data, max_seq_length=MAX_SEQ_LENGTH):\n",
    "    ''' Create the features for the BERT model '''\n",
    "    \n",
    "    print('Processing labels')\n",
    "    #label_list = [get_labels(data, i) for i in range(len(data))]\n",
    "    label_list = data[label_columns].values.tolist()\n",
    "    print('Processing examples')\n",
    "    examples = [InputExample(i, data.iloc[i].Text, labels=label_list[i]) for i in range(len(data))]\n",
    "    print('Converting examples to features')\n",
    "\n",
    "    return convert_examples_to_features(examples, label_list, max_seq_length)\n",
    "\n",
    "def get_features(data, filename):\n",
    "    ''' Create the features to the model '''\n",
    "    if file_exists(filename):\n",
    "        # Use the cached features if it exists\n",
    "        features = load_object(filename)\n",
    "    else:\n",
    "        # Create and save the features\n",
    "        features = create_features(data)\n",
    "        save_object(filename, features)\n",
    "\n",
    "    return features\n",
    "\n",
    "train_features = get_features(train, 'train_bert_features.pkl')\n",
    "test_features = get_features(test, 'test_bert_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPqLF-ikWCAW"
   },
   "outputs": [],
   "source": [
    "batch_num = 32\n",
    "\n",
    "def create_input_ids_tensor(data_features):\n",
    "    return torch.tensor([i.input_ids for i in data_features])\n",
    "\n",
    "def create_input_masks_tensor(data_features):\n",
    "    return torch.tensor([i.input_mask for i in data_features])\n",
    "\n",
    "def create_segment_ids_tensor(data_features):\n",
    "    return torch.tensor([i.segment_ids for i in data_features])\n",
    "\n",
    "def create_label_ids_tensor(data_features):\n",
    "    return torch.tensor([i.label_ids for i in data_features])\n",
    "\n",
    "def create_data_loader(data_features, batch_size=batch_num, drop_last=True, shuffle=True):\n",
    "    dataset = TensorDataset(\n",
    "      create_input_ids_tensor(data_features),\n",
    "      create_input_masks_tensor(data_features),\n",
    "      create_segment_ids_tensor(data_features),\n",
    "      create_label_ids_tensor(data_features))\n",
    "    sampler = RandomSampler(dataset)\n",
    "\n",
    "    return DataLoader(dataset, sampler=sampler, batch_size=batch_size,drop_last=drop_last) if shuffle else DataLoader(dataset, batch_size=batch_size,drop_last=drop_last, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lGl6pNGteBoJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultiLabelSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
       "  (loss_fct): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weights = 'bert-base-uncased'\n",
    "model = BertForMultiLabelSequenceClassification.from_pretrained(pretrained_weights, \n",
    "                                                                num_labels=len(train.columns) - 2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9mLA1Oug7iL"
   },
   "outputs": [],
   "source": [
    "# Set epoch and grad max num\n",
    "epochs = 5\n",
    "max_grad_norm = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yg9g2EWEhXWw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    betas: (0.9, 0.999)\n",
       "    correct_bias: True\n",
       "    eps: 1e-06\n",
       "    lr: 3e-05\n",
       "    weight_decay: 0.0\n",
       "    weight_decay_rate: 0.01\n",
       "\n",
       "Parameter Group 1\n",
       "    betas: (0.9, 0.999)\n",
       "    correct_bias: True\n",
       "    eps: 1e-06\n",
       "    lr: 3e-05\n",
       "    weight_decay: 0.0\n",
       "    weight_decay_rate: 0.0\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine tune model all layer parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "      'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "      'weight_decay_rate': 0.0}\n",
    "]\n",
    "    \n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Y0goKuhf_K7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultiLabelSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
       "  (loss_fct): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare the model for training\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "def calculate_loss(model):\n",
    "    ''' Calculate the loss on the test set for the model '''\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        num_examples = 0\n",
    "\n",
    "        for steps, batch in enumerate(create_data_loader(test_features)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            batch_input_ids, batch_input_masks, batch_segment_ids, batch_labels = batch\n",
    "            loss = model(batch_input_ids, batch_segment_ids, batch_input_masks, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "            num_examples += batch_input_ids.size(0)\n",
    "\n",
    "        return total_loss / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "excBpkTgiHHo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|        | 1/5 [5:37:17<22:29:11, 20237.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0026705804975672553, test loss: 0.0023630357706809135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|      | 2/5 [11:14:43<16:52:00, 20240.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0020954368177045026, test loss: 0.0022941292710932535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|    | 3/5 [16:52:09<11:14:43, 20241.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0017869392465160684, test loss: 0.002323190247547505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|  | 4/5 [22:29:36<5:37:23, 20243.39s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0015305038314823987, test loss: 0.0024752737428435146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|| 5/5 [28:07:10<00:00, 20246.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0013094415359667542, test loss: 0.0026083999580203145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main Training loop \n",
    "for epoch in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    num_examples, num_steps = 0, 0\n",
    "    \n",
    "    # Loop over the batches\n",
    "    for step, batch in enumerate(create_data_loader(train_features)):\n",
    "        logger.info(f'Starting step {step} from epoch {epoch}')\n",
    "        \n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_masks, b_segment_ids, b_labels = batch\n",
    "        outputs = model(b_input_ids, b_segment_ids, b_input_masks, b_labels)\n",
    "        loss = outputs\n",
    "        \n",
    "        if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        num_steps += 1\n",
    "        num_examples += b_input_ids.size(0)\n",
    "                \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        logger.info(f'Finished step {step}')\n",
    "    \n",
    "    # Accumulate the losses\n",
    "    train_losses.append(tr_loss / num_examples)\n",
    "    test_losses.append(calculate_loss(model))\n",
    "    \n",
    "    # Save the partial results\n",
    "    save_object('train_losses_bert.pkl', train_losses)\n",
    "    save_object('test_losses_bert.pkl', test_losses)\n",
    "    \n",
    "    # Display the losses for the epoch\n",
    "    logger.info(f'Train loss: {train_losses[-1]}, test loss: {test_losses[-1]}') \n",
    "    print(f'Train loss: {train_losses[-1]}, test loss: {test_losses[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYlo-OFWu4qx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fda5b7ccf90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'x':list(range(1, len(train_losses) + 1)), 'train':train_losses, 'test':test_losses}).plot(x='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4nuVx6o4vDBC"
   },
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(features, model, batch_size):\n",
    "    ''' Calculate the confusion matrix for the labels of the model '''\n",
    "    \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(create_data_loader(features)):\n",
    "            # add batch to gpu\n",
    "            batch = tuple(t.to('cuda') for t in batch)\n",
    "            b_input_ids, b_input_masks, b_segment_ids, b_labels = batch\n",
    "        \n",
    "            # forward pass\n",
    "            raw_outputs = model(b_input_ids, b_segment_ids, b_input_masks)\n",
    "            predictions.append((torch.sigmoid(raw_outputs[2]) > .5).cpu().numpy().astype(int))\n",
    "            true_labels.append(b_labels.cpu().numpy().astype(int))\n",
    "\n",
    "        true_labels = np.vstack(true_labels)[0:(batch_size * len(predictions)),]\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        return (multilabel_confusion_matrix(true_labels, predictions),\n",
    "                true_labels,\n",
    "                predictions)\n",
    "    \n",
    "def print_pct_correct(features, model):\n",
    "    tmp = calculate_confusion_matrix(features, model, 80)\n",
    "    print(f'Percentage correct {((tmp[1] == tmp[2]).astype(int).sum(1) == 12).sum() / tmp[1].shape[0]}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xwHSqhXwvG7I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage correct 0.8731102881493507\n"
     ]
    }
   ],
   "source": [
    "tmp = calculate_confusion_matrix(train_features, model, 80)\n",
    "print(f'Percentage correct {((tmp[1] == tmp[2]).astype(int).sum(1) == 12).sum() / tmp[1].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BlgUyxacvKVn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage correct 0.7533989448051948\n"
     ]
    }
   ],
   "source": [
    "tmp = calculate_confusion_matrix(test_features, model, 80)\n",
    "print(f'Percentage correct {((tmp[1] == tmp[2]).astype(int).sum(1) == 12).sum() / tmp[1].shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model, '../bert/bert.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename, folder=root_folder):\n",
    "    ''' Load a saved model '''\n",
    "    return torch.load(os.path.join(folder, filename))\n",
    "\n",
    "def calculate_confusion_matrix(features, model, batch_size):\n",
    "    ''' Calculate the confusion matrix from a trained model '''\n",
    "    model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(create_data_loader(features, batch_size, False, False)):\n",
    "            # add batch to gpu\n",
    "            if n_gpu != 0:\n",
    "                batch = tuple(t.to('cuda') for t in batch)\n",
    "            \n",
    "            logger.info(f'Step {step}')\n",
    "            b_input_ids, b_input_masks, b_segment_ids, b_labels = batch\n",
    "        \n",
    "            # forward pass\n",
    "            raw_outputs = model(b_input_ids, b_segment_ids, b_input_masks)\n",
    "            predictions.append((torch.sigmoid(raw_outputs[2]) > .5).cpu().numpy().astype(int))\n",
    "            true_labels.append(b_labels.cpu().numpy().astype(int))\n",
    "\n",
    "        true_labels = np.vstack(true_labels)[0:(batch_size * len(predictions)),]\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        return (multilabel_confusion_matrix(true_labels, predictions),\n",
    "                true_labels,\n",
    "                predictions)\n",
    "\n",
    "def calculate_predictions(features, model, batch_size):\n",
    "    ''' Calculate the prediction from a trained model '''\n",
    "    \n",
    "    if n_gpu != 0:\n",
    "        model.cuda()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()\n",
    "    result = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(create_data_loader(features, batch_size, False, False)):\n",
    "            logger.info(f'Step {step}')\n",
    "            \n",
    "            # add batch to gpu\n",
    "            if n_gpu != 0:\n",
    "                batch = tuple(t.to('cuda') for t in batch)\n",
    "                \n",
    "            b_input_ids, b_input_masks, b_segment_ids, b_labels = batch\n",
    "        \n",
    "            # forward pass\n",
    "            raw_outputs = model(b_input_ids, b_segment_ids, b_input_masks)\n",
    "            predictions = torch.sigmoid(raw_outputs[2]).cpu()\n",
    "            predictions = predictions.numpy()\n",
    "            result = predictions if result is None else np.append(result, predictions, axis=0)\n",
    "            \n",
    "        return result\n",
    "\n",
    "def get_label_columns(data_frame):\n",
    "    ''' Return the columns with labels from the dataframe '''\n",
    "    return [c for c in data_frame.columns if c not in ('Id', 'Text') and '_Pred' not in c]\n",
    "\n",
    "def add_predictions(data_frame, features, model, batch_size):\n",
    "    ''' Calculate and add the predicted values to the dataframe '''\n",
    "    logits = calculate_predictions(features, model, batch_size)\n",
    "    predictions = (logits > .5).astype(int)\n",
    "    \n",
    "    for i, column_name in enumerate(get_label_columns(data_frame)):\n",
    "        data_frame[f'{column_name}_Pred'] = predictions[:, i]\n",
    "\n",
    "def add_predictions_to_dataframe(data_frame, predictions):\n",
    "    ''' Add the model prediction to the dataframe '''\n",
    "    for i, column_name in enumerate(get_label_columns(data_frame)):\n",
    "        data_frame[f'{column_name}_Pred'] = predictions[:, i]\n",
    "        \n",
    "class ModelResult:\n",
    "    ''' Helper class used to make prediction from a trained model '''\n",
    "    \n",
    "    def __init__(self, data_frame, features, model, batch_size):\n",
    "        confusion_matrix, labels, predictions = calculate_confusion_matrix(features, model, batch_size)\n",
    "        self.confusion_matrix = confusion_matrix\n",
    "        self.labels = labels\n",
    "        self.predictions = predictions\n",
    "        self.label_columns = get_label_columns(data_frame)\n",
    "        \n",
    "    def save(self, filename):\n",
    "        ''' Save the model results (this object) to disk '''\n",
    "        save_object(filename, self)\n",
    "        \n",
    "    def get_classification_report(self, output_dictionary):\n",
    "        ''' Return the classification report from the model predictions '''\n",
    "        return classification_report(self.labels, self.predictions, \n",
    "                                     target_names=self.label_columns, \n",
    "                                     output_dict=output_dictionary)\n",
    "        \n",
    "    def get_results_dataframe(self):\n",
    "        ''' Create a summary dataframe with the results from the model '''\n",
    "        report_dict = self.get_classification_report(True)\n",
    "        label_columns = list(self.label_columns)\n",
    "        label_columns.append('weighted avg')\n",
    "        classification_results = pd.DataFrame({'Label':label_columns})\n",
    "        classification_results['Precision'] = [report_dict[c]['precision'] for c in label_columns]\n",
    "        classification_results['Recall'] = [report_dict[c]['recall'] for c in label_columns]\n",
    "        classification_results['F1-Score'] = [report_dict[c]['f1-score'] for c in label_columns]\n",
    "        classification_results['Support'] = [report_dict[c]['support'] for c in label_columns]\n",
    "        classification_results.loc[classification_results['Label'] == 'weighted avg', 'Label'] = 'Weighted Average'\n",
    "        \n",
    "        return classification_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model\n",
    "model = load_model('bert.pt', '../bert')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model_results = ModelResult(test, test_features, model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.save('bert_result_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_bert = load_object('bert_result_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marceloblinder/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "add_predictions_to_dataframe(test, model_results.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9de7f64850>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test[get_label_columns(test)].sum(1).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World</td>\n",
       "      <td>0.869818</td>\n",
       "      <td>0.855948</td>\n",
       "      <td>0.862827</td>\n",
       "      <td>24144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Washington</td>\n",
       "      <td>0.726060</td>\n",
       "      <td>0.667735</td>\n",
       "      <td>0.695677</td>\n",
       "      <td>9050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New_York_and_Region</td>\n",
       "      <td>0.933312</td>\n",
       "      <td>0.885520</td>\n",
       "      <td>0.908788</td>\n",
       "      <td>35718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Front_Page</td>\n",
       "      <td>0.515350</td>\n",
       "      <td>0.477708</td>\n",
       "      <td>0.495816</td>\n",
       "      <td>5271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>0.966815</td>\n",
       "      <td>0.959684</td>\n",
       "      <td>0.963236</td>\n",
       "      <td>43382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US</td>\n",
       "      <td>0.753729</td>\n",
       "      <td>0.783843</td>\n",
       "      <td>0.768491</td>\n",
       "      <td>34040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sports</td>\n",
       "      <td>0.978007</td>\n",
       "      <td>0.976957</td>\n",
       "      <td>0.977482</td>\n",
       "      <td>33546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Obituaries</td>\n",
       "      <td>0.973543</td>\n",
       "      <td>0.992539</td>\n",
       "      <td>0.982950</td>\n",
       "      <td>6970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Health</td>\n",
       "      <td>0.728869</td>\n",
       "      <td>0.685143</td>\n",
       "      <td>0.706330</td>\n",
       "      <td>7451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Education</td>\n",
       "      <td>0.708691</td>\n",
       "      <td>0.531519</td>\n",
       "      <td>0.607450</td>\n",
       "      <td>1396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Science</td>\n",
       "      <td>0.793163</td>\n",
       "      <td>0.655042</td>\n",
       "      <td>0.717516</td>\n",
       "      <td>4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Technology</td>\n",
       "      <td>0.719167</td>\n",
       "      <td>0.709412</td>\n",
       "      <td>0.714256</td>\n",
       "      <td>2433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Weighted Average</td>\n",
       "      <td>0.878209</td>\n",
       "      <td>0.863113</td>\n",
       "      <td>0.870178</td>\n",
       "      <td>207616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Label  Precision    Recall  F1-Score  Support\n",
       "0                 World   0.869818  0.855948  0.862827    24144\n",
       "1            Washington   0.726060  0.667735  0.695677     9050\n",
       "2   New_York_and_Region   0.933312  0.885520  0.908788    35718\n",
       "3            Front_Page   0.515350  0.477708  0.495816     5271\n",
       "4              Business   0.966815  0.959684  0.963236    43382\n",
       "5                    US   0.753729  0.783843  0.768491    34040\n",
       "6                Sports   0.978007  0.976957  0.977482    33546\n",
       "7            Obituaries   0.973543  0.992539  0.982950     6970\n",
       "8                Health   0.728869  0.685143  0.706330     7451\n",
       "9             Education   0.708691  0.531519  0.607450     1396\n",
       "10              Science   0.793163  0.655042  0.717516     4215\n",
       "11           Technology   0.719167  0.709412  0.714256     2433\n",
       "12     Weighted Average   0.878209  0.863113  0.870178   207616"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results.get_results_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14cda0b10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_results.get_results_dataframe().plot.barh(x='Label', y=['Precision', 'Recall', 'F1-Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9de62b6c10>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({'x':((model_results.labels == model_results.predictions).astype(int).sum(1))}).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage correct 0.7459677419354839\n"
     ]
    }
   ],
   "source": [
    "tmp = calculate_confusion_matrix(test_features[0:1000], model, 80)\n",
    "print(f'Percentage correct {((tmp[1] == tmp[2]).astype(int).sum(1) == 12).sum() / tmp[1].shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ns2V0n20FRwh"
   },
   "source": [
    "## Saving the model\n",
    "\n",
    "* [pytorch](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    "\n",
    "* [Google Drive](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=D78AM1fFt2ty)\n",
    "\n",
    "## Bert\n",
    "\n",
    "* [Bert Tutorial (Hidden Layers)](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)\n",
    "\n",
    "* [GIT Huggingface](https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TorchBert 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
