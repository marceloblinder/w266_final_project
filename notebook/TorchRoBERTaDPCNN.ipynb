{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "Sz5dMqVoLWQA",
    "outputId": "8ae43beb-a0b1-4d9c-983e-91f00c2e11a5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import logging\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "from transformers.modeling_bert import BertEmbeddings, BertLayerNorm, BertModel, BertPreTrainedModel, gelu\n",
    "from transformers import *\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    filename='roberta_dpcnn.log')\n",
    "logger = logging.getLogger('roberta_dpcnn.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FN0GUySc2zxf",
    "outputId": "76915fd6-da91-4953-e73c-1c98331149fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions to load and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgJMUdJuCavo"
   },
   "outputs": [],
   "source": [
    "root_folder = '../bert'\n",
    "\n",
    "def save_model(model, model_filename, folder=root_folder):\n",
    "    ''' Save a pytorch model '''\n",
    "    model_file = os.path.join(folder, model_filename)\n",
    "\n",
    "    if os.path.exists(model_file):\n",
    "        os.remove(model_file)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "\n",
    "def load_model(model_filename, folder=root_folder):\n",
    "    ''' Load a pytorch model '''\n",
    "    model_file = os.path.join(folder, model_filename)\n",
    "    model = torch.load(model_filename)\n",
    "\n",
    "    if is_cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    return model\n",
    "\n",
    "def remove_last_model(folder, model_filename, config_filename):\n",
    "    model_file = os.path.join(folder, model_filename)\n",
    "\n",
    "    if os.path.exists(model_file):\n",
    "        os.remove(model_file)\n",
    "\n",
    "    config_file = os.path.join(folder, config_filename)\n",
    "\n",
    "    if os.path.exists(config_file):\n",
    "        os.remove(config_file)\n",
    "\n",
    "def file_exists(filename, folder=root_folder):\n",
    "    return os.path.exists(os.path.join(folder, filename))\n",
    "\n",
    "def save_object(filename, obj, folder=root_folder):\n",
    "    obj_file = os.path.join(folder, filename)\n",
    "\n",
    "    if os.path.exists(obj_file):\n",
    "        os.remove(obj_file)\n",
    "\n",
    "    with open(obj_file, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_object(filename, folder=root_folder):\n",
    "    object_file = os.path.join(folder, filename)\n",
    "\n",
    "    with open(object_file, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoBERTa model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "z3fJqCe-puL0",
    "outputId": "492ad161-0504-44d3-b55e-f671bf69d91d"
   },
   "outputs": [],
   "source": [
    "model_class = RobertaModel\n",
    "tokenizer_class = RobertaTokenizer\n",
    "pretrained_weights = 'roberta-base'\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input (text) processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPPFrVtV996R"
   },
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\"\"\"\n",
    "        self.text_a = text_a\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUKr9OU59-PE"
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, tokenizer, max_seq_length):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    features = []\n",
    "    count = 0\n",
    "\n",
    "    for example in examples:\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "        tokens_b = None\n",
    "\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        tokens = ['<s>'] + tokens_a + ['</s>']\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        \n",
    "        labels_ids = [float(label) for label in example.labels]\n",
    "        input_features = InputFeatures(input_ids, labels_ids)\n",
    "        features.append(input_features)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GZjUNwB3aRl"
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/nyt_full.parquet.gz')\n",
    "SEED = 17\n",
    "train, test = train_test_split(df, test_size=.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Uq4Q6rD_3lJV",
    "outputId": "0642bb9d-0978-47d7-b49e-f8579a6aeec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630802\n",
      "157701\n"
     ]
    }
   ],
   "source": [
    "print(len(train.Id))\n",
    "print(len(test.Id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZG2AW2naOOmT"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 100\n",
    "label_columns = [c for c in train.columns if c not in ['Id', 'Text']]\n",
    "\n",
    "def get_labels(data, i):\n",
    "    return data.iloc[i][label_columns]\n",
    "\n",
    "def create_features(data, tokenizer, max_seq_length=MAX_SEQ_LENGTH):\n",
    "    logger.info('Processing labels')\n",
    "    label_list = data[label_columns].values.tolist()\n",
    "    logger.info('Processing examples')\n",
    "    examples = (InputExample(i, data.iloc[i].Text, labels=label_list[i]) for i in range(len(data)))\n",
    "    logger.info('Converting examples to features')\n",
    "\n",
    "    return convert_examples_to_features(examples, label_list, tokenizer, max_seq_length)\n",
    "\n",
    "def get_features(data, filename):\n",
    "    if file_exists(filename):\n",
    "        features = load_object(filename)\n",
    "    else:\n",
    "        tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "        features = create_features(data, tokenizer)\n",
    "        save_object(filename, features)\n",
    "\n",
    "    return features\n",
    "\n",
    "train_features = get_features(train, 'train_roberta_features.pkl')\n",
    "test_features = get_features(test, 'test_roberta_features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4AjFtP_81D1"
   },
   "outputs": [],
   "source": [
    "class DPCNN(nn.Module):\n",
    "    def __init__(self, num_labels, channel_size=250, width=768):\n",
    "        super(DPCNN, self).__init__()\n",
    "        self.conv_embedding = nn.Conv2d(1, channel_size, (3, width))\n",
    "        self.conv2 = nn.Conv2d(channel_size, channel_size, (3, 1))\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=(3,1), stride=2)\n",
    "        self.padding_conv = nn.ZeroPad2d((0, 0, 1, 1))\n",
    "        self.padding_pool = nn.ZeroPad2d((0, 0, 0, 1))\n",
    "        self.activation_function = nn.ReLU()\n",
    "        self.last_linear = nn.Linear(channel_size, num_labels)\n",
    "        self.channel_size = channel_size\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Expected 4-dimensional input for 4-dimensional weight 3 1 3 768, but got 2-dimensional input of size [32, 768] instead\n",
    "        batch_size, width, height = embeddings.shape\n",
    "\n",
    "        # First transform the BERT embeddings (batch_size, num_characters, 768),\n",
    "        # like (64, 80, 768)\n",
    "        # to a 4D tensor like [64, 1, 80, 768] (required by the Conv2d)\n",
    "        x = embeddings.view((batch_size, 1, width, height))\n",
    "\n",
    "        # Run the first convolution (embedding). The output is [64, 250, 78, 1] \n",
    "        x = self.conv_embedding(x)\n",
    "        #print(f'1 {x.shape}')        \n",
    "        #x = self.activation_function(x)\n",
    "        x_save = x\n",
    "\n",
    "        # Run the second convolution. The output is [64, 250, 76, 1]\n",
    "        x = self.conv2(x)\n",
    "        #print(f'2 {x.shape}')\n",
    "\n",
    "        # Add padding at starting and ending rows of the tensor. After that the\n",
    "        # shape will be [64, 250, 78, 1]\n",
    "        x = self.padding_conv(x)\n",
    "        #x = self.activation_function(x)\n",
    "\n",
    "        # Run another convolution. After that the shape will be [64, 250, 76, 1]\n",
    "        x = self.conv2(x)\n",
    "        x = self.padding_conv(x)\n",
    "        x = x + self.activation_function(x_save)\n",
    "        #print(f'3 {x.shape}')\n",
    "\n",
    "        # Go over the blocks\n",
    "        while x.shape[-2] >= 2:\n",
    "            #print(x.shape)\n",
    "            x = self.padding_pool(x)\n",
    "            #print(f'3.1 {x.shape}')\n",
    "\n",
    "            # Save the pool output to add that to the convolutions at the end\n",
    "            pooling_x = self.pooling(x) \n",
    "            #print(f'pooling {pooling_x.shape}')\n",
    "\n",
    "            # Perform the first convolution\n",
    "            x = self.padding_conv(pooling_x)\n",
    "            x = nn.functional.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            #print(f'4 {x.shape}')\n",
    "\n",
    "            # Perform the second convolution\n",
    "            x = self.padding_conv(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            #print(f'5 {x.shape}')\n",
    "\n",
    "            # Do the addition \n",
    "            x = x + pooling_x\n",
    "\n",
    "        x = x.view(batch_size, self.channel_size)\n",
    "        x = self.last_linear(x)\n",
    "         \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6o0cIOz2nyf"
   },
   "outputs": [],
   "source": [
    "class RoBERTaFull(nn.Module):\n",
    "    \"\"\"BERT model for multiple label classification.\"\"\"\n",
    "    def __init__(self, num_labels, hidden_dropout_prob=.5):\n",
    "        super(RoBERTaFull, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.loss_fct = BCEWithLogitsLoss()\n",
    "\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.dropout = torch.nn.Dropout(hidden_dropout_prob)\n",
    "        self.dpcnn = DPCNN(num_labels)\n",
    "        self.classifier = torch.nn.Linear(num_labels, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        hidden, pooled_output = self.roberta(input_ids)\n",
    "        x = self.dropout(hidden) # pooled_output\n",
    "        x = self.dpcnn(x)\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        if labels is not None: \n",
    "            # Training the model     \n",
    "            return self.loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "        \n",
    "        else:\n",
    "            return hidden, pooled_output, logits\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Loader for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPqLF-ikWCAW"
   },
   "outputs": [],
   "source": [
    "batch_num = 40 #48\n",
    "\n",
    "def create_input_ids_tensor(data_features):\n",
    "    return torch.tensor([i.input_ids for i in data_features])\n",
    "\n",
    "def create_input_masks_tensor(data_features):\n",
    "    return torch.tensor([i.input_mask for i in data_features])\n",
    "\n",
    "def create_segment_ids_tensor(data_features):\n",
    "    return torch.tensor([i.segment_ids for i in data_features])\n",
    "\n",
    "def create_label_ids_tensor(data_features):\n",
    "    return torch.tensor([i.label_ids for i in data_features])\n",
    "\n",
    "def create_data_loader(data_features, batch_size=batch_num, drop_last=True, shuffle=True):\n",
    "    dataset = TensorDataset(\n",
    "      create_input_ids_tensor(data_features),\n",
    "      create_label_ids_tensor(data_features))\n",
    "    sampler = RandomSampler(dataset)\n",
    "\n",
    "    return DataLoader(dataset, sampler=sampler, batch_size=batch_size,drop_last=drop_last) if shuffle else DataLoader(dataset, batch_size=batch_size,drop_last=drop_last, shuffle=shuffle)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "lGl6pNGteBoJ",
    "outputId": "5b99b946-7416-4405-8ea4-c43dcc09f2d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoBERTaFull(\n",
       "  (loss_fct): BCEWithLogitsLoss()\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (dpcnn): DPCNN(\n",
       "    (conv_embedding): Conv2d(1, 250, kernel_size=(3, 768), stride=(1, 1))\n",
       "    (conv2): Conv2d(250, 250, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (pooling): MaxPool2d(kernel_size=(3, 1), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (padding_conv): ZeroPad2d(padding=(0, 0, 1, 1), value=0.0)\n",
       "    (padding_pool): ZeroPad2d(padding=(0, 0, 0, 1), value=0.0)\n",
       "    (activation_function): ReLU()\n",
       "    (last_linear): Linear(in_features=250, out_features=12, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=12, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RoBERTaFull(len(train.columns) - 2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9mLA1Oug7iL"
   },
   "outputs": [],
   "source": [
    "# Set epoch and grad max num\n",
    "epochs = 5\n",
    "max_grad_norm = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Yg9g2EWEhXWw",
    "outputId": "6001ea12-9b73-44a9-d45b-400aef0a6380"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoBERTaFull(\n",
       "  (loss_fct): BCEWithLogitsLoss()\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (dpcnn): DPCNN(\n",
       "    (conv_embedding): Conv2d(1, 250, kernel_size=(3, 768), stride=(1, 1))\n",
       "    (conv2): Conv2d(250, 250, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (pooling): MaxPool2d(kernel_size=(3, 1), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (padding_conv): ZeroPad2d(padding=(0, 0, 1, 1), value=0.0)\n",
       "    (padding_pool): ZeroPad2d(padding=(0, 0, 0, 1), value=0.0)\n",
       "    (activation_function): ReLU()\n",
       "    (last_linear): Linear(in_features=250, out_features=12, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=12, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine tune model all layer parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "      'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "      'weight_decay_rate': 0.0}\n",
    "]\n",
    "    \n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Y0goKuhf_K7"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "def calculate_loss(model):\n",
    "    ''' Calculate the loss on the test set for the model '''\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        num_examples = 0\n",
    "\n",
    "        for steps, batch in enumerate(create_data_loader(test_features)):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            batch_input_ids, batch_labels = batch\n",
    "            loss = model(batch_input_ids, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "            num_examples += batch_input_ids.size(0)\n",
    "\n",
    "        return total_loss / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "excBpkTgiHHo",
    "outputId": "97ddea63-4755-444d-ebdc-37eb98539980"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0019880028410661746, test loss: 0.0018522820536700233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 1/5 [8:22:24<33:29:37, 30144.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0015656202293790112, test loss: 0.001752384035623165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 2/5 [16:44:57<25:07:20, 30146.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.001400121854417104, test loss: 0.0017019517441435642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 3/5 [25:07:35<16:45:00, 30150.19s/it]"
     ]
    }
   ],
   "source": [
    "# Main Training loop \n",
    "for i in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    num_examples, num_steps = 0, 0\n",
    "    \n",
    "    # Loop over the batches    \n",
    "    for step, batch in enumerate(create_data_loader(train_features)):\n",
    "        logger.info(f'Starting step {step}')\n",
    "        \n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_labels = batch\n",
    "        \n",
    "        # forward pass\n",
    "        outputs= model(b_input_ids, b_labels)\n",
    "        loss = outputs\n",
    "        \n",
    "        if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        num_examples += b_input_ids.size(0)\n",
    "        num_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # print train loss per epoch\n",
    "    train_losses.append(tr_loss / num_examples)\n",
    "    test_losses.append(calculate_loss(model))\n",
    "    save_object('train_losses_roberta_dpcnn.pkl', train_losses)\n",
    "    save_object('test_losses_roberta_dpcnn.pkl', test_losses)\n",
    "    \n",
    "    # Display the epoch results\n",
    "    logger.info(f'Train loss: {train_losses[-1]}, test loss: {test_losses[-1]}') \n",
    "    print(f'Train loss: {train_losses[-1]}, test loss: {test_losses[-1]}')\n",
    "    logger.info(f'Saving the model for the epoch {i}')\n",
    "    \n",
    "    # Save the model\n",
    "    save_model(model, 'roberta_dpcnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'x':list(range(1, len(train_losses) + 1)), 'train':train_losses, 'dev':test_losses}).plot(x='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(features, model, batch_size):\n",
    "    ''' Calculate the confusion matrix for the labels of the model '''\n",
    "    \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(create_data_loader(features)):\n",
    "            # add batch to gpu\n",
    "            batch = tuple(t.to('cuda') for t in batch)\n",
    "            b_input_ids, b_labels = batch\n",
    "        \n",
    "            # forward pass\n",
    "            raw_outputs = model(b_input_ids)\n",
    "            predictions.append((torch.sigmoid(raw_outputs[2]) > .5).cpu().numpy().astype(int))\n",
    "            true_labels.append(b_labels.cpu().numpy().astype(int))\n",
    "\n",
    "        true_labels = np.vstack(true_labels)[0:(batch_size * len(predictions)),]\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        return (multilabel_confusion_matrix(true_labels, predictions),\n",
    "                true_labels,\n",
    "                predictions)\n",
    "    \n",
    "def print_pct_correct(features, model):\n",
    "    tmp = calculate_confusion_matrix(features, model, 80)\n",
    "    print(f'Percentage correct {((tmp[1] == tmp[2]).astype(int).sum(1) == 12).sum() / tmp[1].shape[0]}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYn6pn4A58nQ"
   },
   "outputs": [],
   "source": [
    "print_pct_correct(train_features, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pct_correct(test_features, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename, folder=root_folder):\n",
    "    ''' Load the save model '''\n",
    "    \n",
    "    model = RoBERTaFull(len(train.columns) - 2)\n",
    "        \n",
    "    if n_gpu != 0:\n",
    "        state = torch.load(os.path.join(folder, filename))        \n",
    "        model.load_state_dict(state)        \n",
    "        model.cuda()\n",
    "    else:\n",
    "        state = torch.load(os.path.join(folder, filename), map_location=torch.device('cpu'))             \n",
    "        model.load_state_dict(state)\n",
    "      \n",
    "    return model\n",
    "\n",
    "def calculate_confusion_matrix(features, model, batch_size):\n",
    "    ''' Calculate the confusion matrix from a trained model '''\n",
    "    \n",
    "    model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(create_data_loader(features, batch_size, False, False)):\n",
    "            # add batch to gpu\n",
    "            if n_gpu != 0:\n",
    "                batch = tuple(t.to('cuda') for t in batch)\n",
    "            \n",
    "            logger.info(f'Step {step}')\n",
    "            b_input_ids, b_labels = batch\n",
    "        \n",
    "            # forward pass\n",
    "            raw_outputs = model(b_input_ids)\n",
    "            predictions.append((torch.sigmoid(raw_outputs[2]) > .5).cpu().numpy().astype(int))\n",
    "            true_labels.append(b_labels.cpu().numpy().astype(int))\n",
    "\n",
    "        true_labels = np.vstack(true_labels)[0:(batch_size * len(predictions)),]\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        return (multilabel_confusion_matrix(true_labels, predictions),\n",
    "                true_labels,\n",
    "                predictions)\n",
    "\n",
    "def calculate_predictions(features, model, batch_size):\n",
    "    ''' Calculate the prediction from a trained model '''\n",
    "    \n",
    "    if n_gpu != 0:\n",
    "        model.cuda()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()\n",
    "    result = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(create_data_loader(features, batch_size, False, False)):\n",
    "            logger.info(f'Step {step}')\n",
    "            \n",
    "            # add batch to gpu\n",
    "            if n_gpu != 0:\n",
    "                batch = tuple(t.to('cuda') for t in batch)\n",
    "                \n",
    "            b_input_ids, b_labels = batch\n",
    "        \n",
    "            # forward pass\n",
    "            raw_outputs = model(b_input_ids)\n",
    "            predictions = torch.sigmoid(raw_outputs[2]).cpu()\n",
    "            predictions = predictions.numpy()\n",
    "            result = predictions if result is None else np.append(result, predictions, axis=0)\n",
    "            \n",
    "        return result\n",
    "\n",
    "def get_label_columns(data_frame):\n",
    "    ''' Return the columns with labels from the dataframe '''\n",
    "    return [c for c in data_frame.columns if c not in ('Id', 'Text') and '_Pred' not in c]\n",
    "\n",
    "def add_predictions(data_frame, features, model, batch_size):\n",
    "    ''' Calculate and add the predicted values to the dataframe '''\n",
    "    \n",
    "    logits = calculate_predictions(features, model, batch_size)\n",
    "    predictions = (logits > .5).astype(int)\n",
    "    \n",
    "    for i, column_name in enumerate(get_label_columns(data_frame)):\n",
    "        data_frame[f'{column_name}_Pred'] = predictions[:, i]\n",
    "\n",
    "def add_predictions_to_dataframe(data_frame, predictions):\n",
    "    ''' Add the model prediction to the dataframe '''\n",
    "    \n",
    "    for i, column_name in enumerate(get_label_columns(data_frame)):\n",
    "        data_frame[f'{column_name}_Pred'] = predictions[:, i]\n",
    "        \n",
    "class ModelResult:\n",
    "    ''' Helper class used to make prediction from a trained model '''\n",
    "    \n",
    "    def __init__(self, data_frame, features, model, batch_size):\n",
    "        confusion_matrix, labels, predictions = calculate_confusion_matrix(features, model, batch_size)\n",
    "        self.confusion_matrix = confusion_matrix\n",
    "        self.labels = labels\n",
    "        self.predictions = predictions\n",
    "        self.label_columns = get_label_columns(data_frame)\n",
    "        \n",
    "    def save(self, filename):\n",
    "        ''' Save the model results (this object) to disk '''\n",
    "        save_object(filename, self)\n",
    "        \n",
    "    def get_classification_report(self, output_dictionary):\n",
    "        ''' Return the classification report from the model predictions '''\n",
    "        \n",
    "        return classification_report(self.labels, self.predictions, \n",
    "                                     target_names=self.label_columns, \n",
    "                                     output_dict=output_dictionary)\n",
    "        \n",
    "    def get_results_dataframe(self):\n",
    "        ''' Create a summary dataframe with the results from the model '''\n",
    "        \n",
    "        report_dict = self.get_classification_report(True)\n",
    "        label_columns = list(self.label_columns)\n",
    "        label_columns.append('weighted avg')\n",
    "        classification_results = pd.DataFrame({'Label':label_columns})\n",
    "        classification_results['Precision'] = [report_dict[c]['precision'] for c in label_columns]\n",
    "        classification_results['Recall'] = [report_dict[c]['recall'] for c in label_columns]\n",
    "        classification_results['F1-Score'] = [report_dict[c]['f1-score'] for c in label_columns]\n",
    "        classification_results['Support'] = [report_dict[c]['support'] for c in label_columns]\n",
    "        classification_results.loc[classification_results['Label'] == 'weighted avg', 'Label'] = 'Weighted Average'\n",
    "        \n",
    "        return classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('roberta_dpcnn.pt', '../bert')\n",
    "model_results = ModelResult(test, test_features, model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.save('roberta_dpcnn_result_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = load_object('roberta_dpcnn_result_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marceloblinder/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "add_predictions_to_dataframe(test, model_results.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9a3d371a90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[get_label_columns(test)].sum(1).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marceloblinder/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World</td>\n",
       "      <td>0.908612</td>\n",
       "      <td>0.842528</td>\n",
       "      <td>0.874323</td>\n",
       "      <td>24144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Washington</td>\n",
       "      <td>0.770925</td>\n",
       "      <td>0.642210</td>\n",
       "      <td>0.700705</td>\n",
       "      <td>9050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New_York_and_Region</td>\n",
       "      <td>0.924300</td>\n",
       "      <td>0.926060</td>\n",
       "      <td>0.925179</td>\n",
       "      <td>35718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Front_Page</td>\n",
       "      <td>0.911794</td>\n",
       "      <td>0.349080</td>\n",
       "      <td>0.504870</td>\n",
       "      <td>5271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>0.976805</td>\n",
       "      <td>0.958116</td>\n",
       "      <td>0.967370</td>\n",
       "      <td>43382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US</td>\n",
       "      <td>0.787249</td>\n",
       "      <td>0.800940</td>\n",
       "      <td>0.794035</td>\n",
       "      <td>34040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sports</td>\n",
       "      <td>0.985738</td>\n",
       "      <td>0.976629</td>\n",
       "      <td>0.981163</td>\n",
       "      <td>33546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Obituaries</td>\n",
       "      <td>0.986974</td>\n",
       "      <td>0.989240</td>\n",
       "      <td>0.988105</td>\n",
       "      <td>6970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Health</td>\n",
       "      <td>0.798665</td>\n",
       "      <td>0.706482</td>\n",
       "      <td>0.749751</td>\n",
       "      <td>7451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Education</td>\n",
       "      <td>0.798017</td>\n",
       "      <td>0.461318</td>\n",
       "      <td>0.584657</td>\n",
       "      <td>1396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Science</td>\n",
       "      <td>0.734465</td>\n",
       "      <td>0.759905</td>\n",
       "      <td>0.746968</td>\n",
       "      <td>4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Technology</td>\n",
       "      <td>0.702630</td>\n",
       "      <td>0.746815</td>\n",
       "      <td>0.724049</td>\n",
       "      <td>2433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Weighted Average</td>\n",
       "      <td>0.904194</td>\n",
       "      <td>0.869321</td>\n",
       "      <td>0.882721</td>\n",
       "      <td>207616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Label  Precision    Recall  F1-Score  Support\n",
       "0                 World   0.908612  0.842528  0.874323    24144\n",
       "1            Washington   0.770925  0.642210  0.700705     9050\n",
       "2   New_York_and_Region   0.924300  0.926060  0.925179    35718\n",
       "3            Front_Page   0.911794  0.349080  0.504870     5271\n",
       "4              Business   0.976805  0.958116  0.967370    43382\n",
       "5                    US   0.787249  0.800940  0.794035    34040\n",
       "6                Sports   0.985738  0.976629  0.981163    33546\n",
       "7            Obituaries   0.986974  0.989240  0.988105     6970\n",
       "8                Health   0.798665  0.706482  0.749751     7451\n",
       "9             Education   0.798017  0.461318  0.584657     1396\n",
       "10              Science   0.734465  0.759905  0.746968     4215\n",
       "11           Technology   0.702630  0.746815  0.724049     2433\n",
       "12     Weighted Average   0.904194  0.869321  0.882721   207616"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results.get_results_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f970f3a6fd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_results.get_results_dataframe().plot.barh(x='Label', y='Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f970513d250>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({'x':((model_results.labels == model_results.predictions).astype(int).sum(1))}).plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_train = ModelResult(train, train_features, model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results_train.save('roberta_dpcnn_result_train.pkl')\n",
    "model_results_train = load_object('roberta_dpcnn_result_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World</td>\n",
       "      <td>0.952800</td>\n",
       "      <td>0.875735</td>\n",
       "      <td>0.912644</td>\n",
       "      <td>96745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Washington</td>\n",
       "      <td>0.804314</td>\n",
       "      <td>0.734513</td>\n",
       "      <td>0.767830</td>\n",
       "      <td>36401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New_York_and_Region</td>\n",
       "      <td>0.952685</td>\n",
       "      <td>0.966140</td>\n",
       "      <td>0.959365</td>\n",
       "      <td>143030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Front_Page</td>\n",
       "      <td>0.895194</td>\n",
       "      <td>0.417614</td>\n",
       "      <td>0.569536</td>\n",
       "      <td>21721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>0.984731</td>\n",
       "      <td>0.981501</td>\n",
       "      <td>0.983114</td>\n",
       "      <td>173797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US</td>\n",
       "      <td>0.820393</td>\n",
       "      <td>0.859255</td>\n",
       "      <td>0.839374</td>\n",
       "      <td>135337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sports</td>\n",
       "      <td>0.993406</td>\n",
       "      <td>0.989079</td>\n",
       "      <td>0.991238</td>\n",
       "      <td>133134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Obituaries</td>\n",
       "      <td>0.989659</td>\n",
       "      <td>0.995051</td>\n",
       "      <td>0.992347</td>\n",
       "      <td>28084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Health</td>\n",
       "      <td>0.860942</td>\n",
       "      <td>0.784468</td>\n",
       "      <td>0.820928</td>\n",
       "      <td>29462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Education</td>\n",
       "      <td>0.878753</td>\n",
       "      <td>0.544519</td>\n",
       "      <td>0.672391</td>\n",
       "      <td>5537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Science</td>\n",
       "      <td>0.842064</td>\n",
       "      <td>0.833144</td>\n",
       "      <td>0.837580</td>\n",
       "      <td>16709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Technology</td>\n",
       "      <td>0.769976</td>\n",
       "      <td>0.832717</td>\n",
       "      <td>0.800118</td>\n",
       "      <td>9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Weighted Average</td>\n",
       "      <td>0.929481</td>\n",
       "      <td>0.908010</td>\n",
       "      <td>0.915790</td>\n",
       "      <td>829689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Label  Precision    Recall  F1-Score  Support\n",
       "0                 World   0.952800  0.875735  0.912644    96745\n",
       "1            Washington   0.804314  0.734513  0.767830    36401\n",
       "2   New_York_and_Region   0.952685  0.966140  0.959365   143030\n",
       "3            Front_Page   0.895194  0.417614  0.569536    21721\n",
       "4              Business   0.984731  0.981501  0.983114   173797\n",
       "5                    US   0.820393  0.859255  0.839374   135337\n",
       "6                Sports   0.993406  0.989079  0.991238   133134\n",
       "7            Obituaries   0.989659  0.995051  0.992347    28084\n",
       "8                Health   0.860942  0.784468  0.820928    29462\n",
       "9             Education   0.878753  0.544519  0.672391     5537\n",
       "10              Science   0.842064  0.833144  0.837580    16709\n",
       "11           Technology   0.769976  0.832717  0.800118     9732\n",
       "12     Weighted Average   0.929481  0.908010  0.915790   829689"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results_train.get_results_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f96ee97c990>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_results_train.get_results_dataframe().plot.barh(x='Label', y='Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f96f09ea9d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({'x':((model_results.labels == model_results.predictions).astype(int).sum(1))}).plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ns2V0n20FRwh"
   },
   "source": [
    "## Saving the model\n",
    "\n",
    "* [pytorch](https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
    "\n",
    "* [Google Drive](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=D78AM1fFt2ty)\n",
    "\n",
    "## Bert\n",
    "\n",
    "* [Bert Tutorial (Hidden Layers)](https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/)\n",
    "\n",
    "* [GIT Huggingface](https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TorchBert 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
