{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12591,
     "status": "ok",
     "timestamp": 1574043893639,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "pYIWtv8Z7YqN",
    "outputId": "769bb331-24e9-419f-a782-b01dbec24eba"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import logging as log\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification, XLNetConfig\n",
    "from transformers import *\n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import multilabel_confusion_matrix, classification_report\n",
    "\n",
    "SEED = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update all modules run:\n",
    "\n",
    "__pip install sentencepiece==0.1.95 pyarrow__\n",
    "\n",
    "__pip3 list --outdated --format=freeze | grep -v '^\\-e' | cut -d = -f 1  | xargs -n1 pip3 install -U__\n",
    "\n",
    "Run that for an error saying *\"ModuleNotFoundError: No module named 'transformers.modeling_xlnet'\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9FgNNg7k4Bc"
   },
   "outputs": [],
   "source": [
    "log.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    filename='xlnet_dpcnn.log',\n",
    "                    level=log.INFO)\n",
    "logger = log.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKBzbugJ4hOi"
   },
   "source": [
    "#### Setting up gpu environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8979,
     "status": "ok",
     "timestamp": 1574043893641,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "m60T5Qio7LCs",
    "outputId": "f93af2ae-63dd-4e96-a31a-eb5f9e910478"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla K80'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAIjuU18JgNK"
   },
   "source": [
    "Preparing NYT dataset for *XLNet*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tRXDaFimVyYO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_full = pd.read_parquet('../data/nyt.2000.parquet.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Rso1WmWelw3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32146, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N71zwRkEn2-J"
   },
   "outputs": [],
   "source": [
    "# sample a small set of observations for testing \n",
    "#df = df_full.sample(10000, random_state=SEED)\n",
    "\n",
    "# full dataset for the real deal\n",
    "df = df_full\n",
    "\n",
    "df = df.set_index(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 967,
     "status": "ok",
     "timestamp": 1574021270971,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "ccpWYcp5YHcH",
    "outputId": "be6ad6df-4ca3-4563-9c22-61e2fa5d7dd7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>World</th>\n",
       "      <th>Washington</th>\n",
       "      <th>New_York_and_Region</th>\n",
       "      <th>Front_Page</th>\n",
       "      <th>Business</th>\n",
       "      <th>US</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Obituaries</th>\n",
       "      <th>Health</th>\n",
       "      <th>Education</th>\n",
       "      <th>Science</th>\n",
       "      <th>Technology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1165059</th>\n",
       "      <td>The man accused of stabbing George Harrison on...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165110</th>\n",
       "      <td>Most nights, P. J. Sanchez, 9, comes into his ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165173</th>\n",
       "      <td>Of all the instincts in animals, the strongest...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165189</th>\n",
       "      <td>THE Scarecrow in \"The Wizard of Oz\" wanted onl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165219</th>\n",
       "      <td>UNLIKE the sheep named Dolly, the mouse named ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Text  World  Washington  \\\n",
       "Id                                                                              \n",
       "1165059  The man accused of stabbing George Harrison on...      1           0   \n",
       "1165110  Most nights, P. J. Sanchez, 9, comes into his ...      0           0   \n",
       "1165173  Of all the instincts in animals, the strongest...      0           0   \n",
       "1165189  THE Scarecrow in \"The Wizard of Oz\" wanted onl...      0           0   \n",
       "1165219  UNLIKE the sheep named Dolly, the mouse named ...      0           0   \n",
       "\n",
       "         New_York_and_Region  Front_Page  Business  US  Sports  Obituaries  \\\n",
       "Id                                                                           \n",
       "1165059                    0           0         0   0       0           0   \n",
       "1165110                    1           0         0   0       0           0   \n",
       "1165173                    0           0         0   0       0           0   \n",
       "1165189                    0           0         0   0       0           0   \n",
       "1165219                    0           0         0   0       0           0   \n",
       "\n",
       "         Health  Education  Science  Technology  \n",
       "Id                                               \n",
       "1165059       1          0        0           0  \n",
       "1165110       1          0        0           0  \n",
       "1165173       1          0        0           0  \n",
       "1165189       1          0        0           0  \n",
       "1165219       1          0        0           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.Health == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3503,
     "status": "ok",
     "timestamp": 1574043897154,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "klfFFgooZsLC",
    "outputId": "aa37830e-076b-4b0a-edd6-7b1f6bded2ac"
   },
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4349,
     "status": "ok",
     "timestamp": 1574021274370,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "-Mc115VSyv1T",
    "outputId": "c323fe88-ce91-40d3-830e-7c20cd528e6c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(\n",
    "    [len(s) for s in df.sample(1000)[\"Text\"].apply(lambda x: tokenizer.tokenize(x))], \n",
    "    bins=[0, 128, 256, 512, 1024, 2048, 5096], \n",
    "    histtype='bar', \n",
    "    facecolor='b',\n",
    "    edgecolor='k',\n",
    "    alpha=0.5)\n",
    "plt.xticks([0, 128, 256, 512, 1024, 2048, 5096], rotation='vertical')\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Sentences')\n",
    "plt.title('Histogram of number of tokens in sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2174909,
     "status": "ok",
     "timestamp": 1574023523372,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "gDOAvSBj9jrP",
    "outputId": "7c436f98-7c5c-4408-a41e-ce86ea766246"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer: 100%|██████████| 32146/32146 [01:07<00:00, 475.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32146,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentences larger than MAX_LEN are truncated (out-of-memory workaround)\n",
    "MAX_LEN = 128\n",
    "\n",
    "df_input_ids_full = df[\"Text\"].copy()\n",
    "XLNET_END_TOKEN_IDS = tokenizer.encode(\"[SEP][CLS]\")\n",
    "\n",
    "for i in tqdm(df_input_ids_full.index, desc=\"Tokenizer\"):\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(\n",
    "      tokenizer.tokenize(df_input_ids_full[i])[:MAX_LEN - len(XLNET_END_TOKEN_IDS)])\n",
    "  df_input_ids_full.at[i] = input_ids + XLNET_END_TOKEN_IDS\n",
    "\n",
    "df_input_ids_full.shape\n",
    "# the code below causes out-of-memory crash and the code above was used\n",
    "\n",
    "# convert to XLNet vocabulary tokens\n",
    "#df[\"tokens\"] = df[\"Text\"].apply(lambda x: tokenizer.tokenize(x + \" [SEP] [CLS]\"))\n",
    "\n",
    "# convert tokens to XLNet vocabulary ids\n",
    "#df[\"input_ids_full\"] = df[\"tokens\"].apply(lambda x: tokenizer.convert_tokens_to_ids(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JABiS_MpzHg3"
   },
   "outputs": [],
   "source": [
    "# using Keras function to handle padding\n",
    "df[\"input_ids\"] = pad_sequences(\n",
    "    df_input_ids_full, \n",
    "    maxlen=MAX_LEN, \n",
    "    dtype=\"long\", \n",
    "    padding=\"post\", \n",
    "    value=0).tolist()\n",
    "\n",
    "# Create XLNet masks: 1 for tokens 0 for padding\n",
    "attention_masks = []\n",
    "for input_id in df.input_ids:\n",
    "  mask = [int(i>0) for i in input_id]\n",
    "  attention_masks.append(mask)\n",
    "df[\"attention_masks\"] = attention_masks\n",
    "\n",
    "# Create XLNet segments: 0 for each token of the first sentence, \n",
    "# followed by a 1 for each token of the second sentence.  For one sentence \n",
    "# inputs, this is simply a sequence of 0s\n",
    "df[\"segment_masks\"] = np.zeros((df.shape[0], MAX_LEN), dtype=int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1574023602658,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "n3mCx93492k6",
    "outputId": "453e7f56-d3fb-4082-97f2-ce1a07a84cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 1165119\n",
      "The hijackers of an Indian Airlines plane won the release of three prominent militants jailed in India. All of them sped away from the airport in Afghanistan, trying to get out of the country. The names and nationalities of the hijackers are still unknown. With the release, more than 150 haggard passengers, kept on board for eight days, were freed unharmed. They flew to New Delhi, for family reunions, both wrenching and joyous.\n",
      "[32, 24362, 20, 48, 1280, 6227, 2352, 282, 18, 947, 20, 139, 3788, 3211, 11733, 25, 837, 9, 394, 20, 107, 24646, 308, 40, 18, 2212, 25, 1805, 19, 619, 22, 133, 78, 20, 18, 234, 9, 32, 1931, 21, 27321, 20, 18, 24362, 41, 194, 4027, 9, 473, 18, 947, 19, 70, 100, 4076, 10118, 299, 11992, 3372, 19, 1340, 31, 1036, 28, 869, 307, 19, 55, 8494, 28510, 9, 200, 5415, 22, 158, 6849, 19, 28, 273, 15459, 23, 19, 207, 17, 29741, 21, 30584, 9, 4145, 83, 8186, 3158, 10849, 7416, 83, 3158, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "98\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "98\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# check for a observation with less than MAX_LEN tokens\n",
    "for i in df.index:\n",
    "  if(np.count_nonzero(df.loc[i, \"input_ids\"]) < 128):\n",
    "    print(\"Index:\", i)\n",
    "    print(df.loc[i, \"Text\"])\n",
    "    print(df.loc[i, \"input_ids\"])\n",
    "    print(np.count_nonzero(df.loc[i, \"input_ids\"]))\n",
    "    print(df.loc[i, \"attention_masks\"])\n",
    "    print(np.count_nonzero(df.loc[i, \"attention_masks\"]))\n",
    "    print(df.loc[i, \"segment_masks\"])\n",
    "    print(np.count_nonzero(df.loc[i, \"segment_masks\"]))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1574023780270,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "xF6iYg5F5E9G",
    "outputId": "8dff47c7-418f-46a9-c519-37165533d078"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>World</th>\n",
       "      <th>Washington</th>\n",
       "      <th>New_York_and_Region</th>\n",
       "      <th>Front_Page</th>\n",
       "      <th>Business</th>\n",
       "      <th>US</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Obituaries</th>\n",
       "      <th>Health</th>\n",
       "      <th>Education</th>\n",
       "      <th>Science</th>\n",
       "      <th>Technology</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_masks</th>\n",
       "      <th>segment_masks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1221402</th>\n",
       "      <td>CAN'T YOU HEAR ME CALLIN'  The Life of Bill Mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[7868, 26, 658, 13938, 732, 15026, 17, 5241, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1220990</th>\n",
       "      <td>For the second time in two years, Darryl Straw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[247, 18, 205, 92, 25, 87, 123, 19, 28242, 290...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169482</th>\n",
       "      <td>A recent study of women on active duty in the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[79, 644, 757, 20, 412, 31, 1768, 3684, 25, 18...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198566</th>\n",
       "      <td>\"Just put the violin on the shoulder, voomp!\"\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[17, 12, 5817, 331, 18, 17680, 31, 18, 2741, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195487</th>\n",
       "      <td>IN Paris recently the signs of spring were eve...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[7039, 1737, 1050, 18, 2832, 20, 1919, 55, 593...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Text  World  Washington  \\\n",
       "Id                                                                              \n",
       "1221402  CAN'T YOU HEAR ME CALLIN'  The Life of Bill Mo...      0           0   \n",
       "1220990  For the second time in two years, Darryl Straw...      0           0   \n",
       "1169482  A recent study of women on active duty in the ...      0           0   \n",
       "1198566  \"Just put the violin on the shoulder, voomp!\"\\...      0           0   \n",
       "1195487  IN Paris recently the signs of spring were eve...      0           0   \n",
       "\n",
       "         New_York_and_Region  Front_Page  Business  US  Sports  Obituaries  \\\n",
       "Id                                                                           \n",
       "1221402                    0           0         0   1       0           0   \n",
       "1220990                    0           0         0   0       1           0   \n",
       "1169482                    0           0         0   0       0           0   \n",
       "1198566                    1           0         0   0       0           0   \n",
       "1195487                    1           0         0   0       0           0   \n",
       "\n",
       "         Health  Education  Science  Technology  \\\n",
       "Id                                                \n",
       "1221402       0          0        0           0   \n",
       "1220990       1          0        0           0   \n",
       "1169482       1          0        0           0   \n",
       "1198566       0          0        0           0   \n",
       "1195487       0          0        0           0   \n",
       "\n",
       "                                                 input_ids  \\\n",
       "Id                                                           \n",
       "1221402  [7868, 26, 658, 13938, 732, 15026, 17, 5241, 2...   \n",
       "1220990  [247, 18, 205, 92, 25, 87, 123, 19, 28242, 290...   \n",
       "1169482  [79, 644, 757, 20, 412, 31, 1768, 3684, 25, 18...   \n",
       "1198566  [17, 12, 5817, 331, 18, 17680, 31, 18, 2741, 1...   \n",
       "1195487  [7039, 1737, 1050, 18, 2832, 20, 1919, 55, 593...   \n",
       "\n",
       "                                           attention_masks  \\\n",
       "Id                                                           \n",
       "1221402  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1220990  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1169482  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1198566  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1195487  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                             segment_masks  \n",
       "Id                                                          \n",
       "1221402  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1220990  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1169482  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1198566  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1195487  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32522,
     "status": "ok",
     "timestamp": 1574043942566,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "a9y5n_oickMe",
    "outputId": "186d3f7f-8b8e-4726-af6d-2b293d780f27"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "MODEL_PATH = \"../xlnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6F9u8iDBLBhH"
   },
   "outputs": [],
   "source": [
    "xlnet_ds_prepared = MODEL_PATH + \"/xlnet_prepared_ds_parquet.gzip\"\n",
    "\n",
    "df.to_parquet(xlnet_ds_prepared, compression='gzip')\n",
    "df = pd.read_parquet(xlnet_ds_prepared)\n",
    "#df = df.sample(50000, random_state = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dvid14fald_t"
   },
   "source": [
    "#### Split training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1834,
     "status": "ok",
     "timestamp": 1574044056959,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "VP0l4vBo_QiL",
    "outputId": "1869086e-bdbb-45a6-9ffa-2f3ef3108111"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25716, 16) (6430, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state = SEED)\n",
    "\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcWkqqj1zZmZ"
   },
   "source": [
    "#### Put data into data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqqlXHl2GmAM"
   },
   "outputs": [],
   "source": [
    "tt_train_input_ids = torch.tensor(train_df[\"input_ids\"].tolist())\n",
    "tt_test_input_ids = torch.tensor(test_df[\"input_ids\"].tolist())\n",
    "\n",
    "tt_train_attention_masks = torch.tensor(train_df[\"attention_masks\"].tolist())\n",
    "tt_test_attention_masks = torch.tensor(test_df[\"attention_masks\"].tolist())\n",
    "\n",
    "tt_train_segment_masks = torch.tensor(train_df[\"segment_masks\"].tolist())\n",
    "tt_test_segment_masks = torch.tensor(test_df[\"segment_masks\"].tolist())\n",
    "\n",
    "label_columns = [\n",
    "  'World',\n",
    "  'Washington',\n",
    "  'New_York_and_Region',\n",
    "  'Front_Page',\n",
    "  'Business',\n",
    "  'US',\n",
    "  'Sports',\n",
    "  'Obituaries',\n",
    "  'Health',\n",
    "  'Education',\n",
    "  'Science',\n",
    "  'Technology']\n",
    "\n",
    "tt_train_labels = torch.tensor(train_df[label_columns].values.astype(np.float))\n",
    "tt_test_labels = torch.tensor(test_df[label_columns].values.astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zIePNKLWzh6N"
   },
   "outputs": [],
   "source": [
    "# training batch size\n",
    "batch_num = 32\n",
    "\n",
    "train_data = TensorDataset(\n",
    "    tt_train_input_ids, \n",
    "    tt_train_attention_masks, \n",
    "    tt_train_segment_masks, \n",
    "    tt_train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)\n",
    "\n",
    "valid_data = TensorDataset(\n",
    "    tt_test_input_ids, \n",
    "    tt_test_attention_masks, \n",
    "    tt_test_segment_masks, \n",
    "    tt_test_labels)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_data, sampler=valid_sampler, batch_size=batch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V6UEl2uP0so5"
   },
   "source": [
    "#### DPCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LuxweYMN0qeJ"
   },
   "outputs": [],
   "source": [
    "class DPCNN(torch.nn.Module):\n",
    "    def __init__(self, num_labels, channel_size=250, width=768):\n",
    "        super(DPCNN, self).__init__()\n",
    "        self.conv_embedding = torch.nn.Conv2d(1, channel_size, (3, width))\n",
    "        self.conv2 = torch.nn.Conv2d(channel_size, channel_size, (3, 1))\n",
    "        self.pooling = torch.nn.MaxPool2d(kernel_size=(3,1), stride=2)\n",
    "        self.padding_conv = torch.nn.ZeroPad2d((0, 0, 1, 1))\n",
    "        self.padding_pool = torch.nn.ZeroPad2d((0, 0, 0, 1))\n",
    "        self.activation_function = torch.nn.ReLU()\n",
    "        self.last_linear = torch.nn.Linear(channel_size, num_labels)\n",
    "        self.channel_size = channel_size\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        \n",
    "        batch_size, width, height = embeddings.shape\n",
    "\n",
    "        # First transform the BERT embeddings (batch_size, num_characters, 768),\n",
    "        # like (64, 80, 768)\n",
    "        # to a 4D tensor like [64, 1, 80, 768] (required by the Conv2d)\n",
    "        x = embeddings.view((batch_size, 1, width, height))\n",
    "\n",
    "        # Run the first convolution (embedding). The output is [64, 250, 78, 1] \n",
    "        x = self.conv_embedding(x)\n",
    "        #print(f'1 {x.shape}')        \n",
    "        #x = self.activation_function(x)\n",
    "        x_save = x\n",
    "\n",
    "        # Run the second convolution. The output is [64, 250, 76, 1]\n",
    "        x = self.conv2(x)\n",
    "        #print(f'2 {x.shape}')\n",
    "\n",
    "        # Add padding at starting and ending rows of the tensor. After that the\n",
    "        # shape will be [64, 250, 78, 1]\n",
    "        x = self.padding_conv(x)\n",
    "        #x = self.activation_function(x)\n",
    "\n",
    "        # Run another convolution. After that the shape will be [64, 250, 76, 1]\n",
    "        x = self.conv2(x)\n",
    "        x = self.padding_conv(x)\n",
    "        x = x + self.activation_function(x_save)\n",
    "        #print(f'3 {x.shape}')\n",
    "\n",
    "        # Go over the blocks\n",
    "        while x.shape[-2] >= 2:\n",
    "            #print(x.shape)\n",
    "            x = self.padding_pool(x)\n",
    "            #print(f'3.1 {x.shape}')\n",
    "\n",
    "            # Save the pool output to add that to the convolutions at the end\n",
    "            pooling_x = self.pooling(x) \n",
    "            #print(f'pooling {pooling_x.shape}')\n",
    "\n",
    "            # Perform the first convolution\n",
    "            x = self.padding_conv(pooling_x)\n",
    "            x = torch.nn.functional.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            #print(f'4 {x.shape}')\n",
    "\n",
    "            # Perform the second convolution\n",
    "            x = self.padding_conv(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.conv2(x)\n",
    "            #print(f'5 {x.shape}')\n",
    "\n",
    "            # Do the addition \n",
    "            x = x + pooling_x\n",
    "\n",
    "        x = x.view(batch_size, self.channel_size)\n",
    "        x = self.last_linear(x)\n",
    "         \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xMtZ8Dk1k7o"
   },
   "outputs": [],
   "source": [
    "class XLNetDPCNN(XLNetPreTrainedModel):\n",
    "    \"\"\"XLNet model for multiple label classification.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(XLNetDPCNN, self).__init__(config)\n",
    "        num_labels = len(label_columns)\n",
    "        self.num_labels = num_labels\n",
    "        self.xlnet = XLNetModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.dropout)\n",
    "        self.dpcnn = DPCNN(num_labels)\n",
    "        self.classifier = torch.nn.Linear(num_labels, num_labels)\n",
    "\n",
    "    def forward(self, \n",
    "                input_ids, \n",
    "                attention_mask=None, \n",
    "                mems=None, \n",
    "                perm_mask=None, \n",
    "                target_mapping=None,\n",
    "                token_type_ids=None, \n",
    "                input_mask=None, \n",
    "                head_mask=None, \n",
    "                labels=None):\n",
    "      \n",
    "        transformer_outputs = self.xlnet(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            mems=mems,\n",
    "            perm_mask=perm_mask,\n",
    "            target_mapping=target_mapping,\n",
    "            token_type_ids=token_type_ids,\n",
    "            input_mask=input_mask,\n",
    "            head_mask=head_mask)\n",
    "\n",
    "        output = transformer_outputs[0]\n",
    "        x = self.dropout(output) \n",
    "        x = self.dpcnn(x)\n",
    "\n",
    "        logits = self.classifier(x)\n",
    "\n",
    "        outputs = (logits,) + transformer_outputs[1:]  # Keep mems, hidden states, attentions if there are in it\n",
    "\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = torch.nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                #  We multilabel multiclass\n",
    "                loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # return (loss), logits, (mems), (hidden states), (attentions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ub-NPcm_vMX-"
   },
   "source": [
    "#### Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SyEyQKcLzlF3"
   },
   "source": [
    "#### Load XLNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3854,
     "status": "ok",
     "timestamp": 1574044421007,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "mrsgCnjPzq1P",
    "outputId": "c7fcdb60-c182-4647-b2f9-f42ab06b2df9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetDPCNN: ['transformer.mask_emb', 'transformer.word_embedding.weight', 'transformer.layer.0.rel_attn.q', 'transformer.layer.0.rel_attn.k', 'transformer.layer.0.rel_attn.v', 'transformer.layer.0.rel_attn.o', 'transformer.layer.0.rel_attn.r', 'transformer.layer.0.rel_attn.r_r_bias', 'transformer.layer.0.rel_attn.r_s_bias', 'transformer.layer.0.rel_attn.r_w_bias', 'transformer.layer.0.rel_attn.seg_embed', 'transformer.layer.0.rel_attn.layer_norm.weight', 'transformer.layer.0.rel_attn.layer_norm.bias', 'transformer.layer.0.ff.layer_norm.weight', 'transformer.layer.0.ff.layer_norm.bias', 'transformer.layer.0.ff.layer_1.weight', 'transformer.layer.0.ff.layer_1.bias', 'transformer.layer.0.ff.layer_2.weight', 'transformer.layer.0.ff.layer_2.bias', 'transformer.layer.1.rel_attn.q', 'transformer.layer.1.rel_attn.k', 'transformer.layer.1.rel_attn.v', 'transformer.layer.1.rel_attn.o', 'transformer.layer.1.rel_attn.r', 'transformer.layer.1.rel_attn.r_r_bias', 'transformer.layer.1.rel_attn.r_s_bias', 'transformer.layer.1.rel_attn.r_w_bias', 'transformer.layer.1.rel_attn.seg_embed', 'transformer.layer.1.rel_attn.layer_norm.weight', 'transformer.layer.1.rel_attn.layer_norm.bias', 'transformer.layer.1.ff.layer_norm.weight', 'transformer.layer.1.ff.layer_norm.bias', 'transformer.layer.1.ff.layer_1.weight', 'transformer.layer.1.ff.layer_1.bias', 'transformer.layer.1.ff.layer_2.weight', 'transformer.layer.1.ff.layer_2.bias', 'transformer.layer.2.rel_attn.q', 'transformer.layer.2.rel_attn.k', 'transformer.layer.2.rel_attn.v', 'transformer.layer.2.rel_attn.o', 'transformer.layer.2.rel_attn.r', 'transformer.layer.2.rel_attn.r_r_bias', 'transformer.layer.2.rel_attn.r_s_bias', 'transformer.layer.2.rel_attn.r_w_bias', 'transformer.layer.2.rel_attn.seg_embed', 'transformer.layer.2.rel_attn.layer_norm.weight', 'transformer.layer.2.rel_attn.layer_norm.bias', 'transformer.layer.2.ff.layer_norm.weight', 'transformer.layer.2.ff.layer_norm.bias', 'transformer.layer.2.ff.layer_1.weight', 'transformer.layer.2.ff.layer_1.bias', 'transformer.layer.2.ff.layer_2.weight', 'transformer.layer.2.ff.layer_2.bias', 'transformer.layer.3.rel_attn.q', 'transformer.layer.3.rel_attn.k', 'transformer.layer.3.rel_attn.v', 'transformer.layer.3.rel_attn.o', 'transformer.layer.3.rel_attn.r', 'transformer.layer.3.rel_attn.r_r_bias', 'transformer.layer.3.rel_attn.r_s_bias', 'transformer.layer.3.rel_attn.r_w_bias', 'transformer.layer.3.rel_attn.seg_embed', 'transformer.layer.3.rel_attn.layer_norm.weight', 'transformer.layer.3.rel_attn.layer_norm.bias', 'transformer.layer.3.ff.layer_norm.weight', 'transformer.layer.3.ff.layer_norm.bias', 'transformer.layer.3.ff.layer_1.weight', 'transformer.layer.3.ff.layer_1.bias', 'transformer.layer.3.ff.layer_2.weight', 'transformer.layer.3.ff.layer_2.bias', 'transformer.layer.4.rel_attn.q', 'transformer.layer.4.rel_attn.k', 'transformer.layer.4.rel_attn.v', 'transformer.layer.4.rel_attn.o', 'transformer.layer.4.rel_attn.r', 'transformer.layer.4.rel_attn.r_r_bias', 'transformer.layer.4.rel_attn.r_s_bias', 'transformer.layer.4.rel_attn.r_w_bias', 'transformer.layer.4.rel_attn.seg_embed', 'transformer.layer.4.rel_attn.layer_norm.weight', 'transformer.layer.4.rel_attn.layer_norm.bias', 'transformer.layer.4.ff.layer_norm.weight', 'transformer.layer.4.ff.layer_norm.bias', 'transformer.layer.4.ff.layer_1.weight', 'transformer.layer.4.ff.layer_1.bias', 'transformer.layer.4.ff.layer_2.weight', 'transformer.layer.4.ff.layer_2.bias', 'transformer.layer.5.rel_attn.q', 'transformer.layer.5.rel_attn.k', 'transformer.layer.5.rel_attn.v', 'transformer.layer.5.rel_attn.o', 'transformer.layer.5.rel_attn.r', 'transformer.layer.5.rel_attn.r_r_bias', 'transformer.layer.5.rel_attn.r_s_bias', 'transformer.layer.5.rel_attn.r_w_bias', 'transformer.layer.5.rel_attn.seg_embed', 'transformer.layer.5.rel_attn.layer_norm.weight', 'transformer.layer.5.rel_attn.layer_norm.bias', 'transformer.layer.5.ff.layer_norm.weight', 'transformer.layer.5.ff.layer_norm.bias', 'transformer.layer.5.ff.layer_1.weight', 'transformer.layer.5.ff.layer_1.bias', 'transformer.layer.5.ff.layer_2.weight', 'transformer.layer.5.ff.layer_2.bias', 'transformer.layer.6.rel_attn.q', 'transformer.layer.6.rel_attn.k', 'transformer.layer.6.rel_attn.v', 'transformer.layer.6.rel_attn.o', 'transformer.layer.6.rel_attn.r', 'transformer.layer.6.rel_attn.r_r_bias', 'transformer.layer.6.rel_attn.r_s_bias', 'transformer.layer.6.rel_attn.r_w_bias', 'transformer.layer.6.rel_attn.seg_embed', 'transformer.layer.6.rel_attn.layer_norm.weight', 'transformer.layer.6.rel_attn.layer_norm.bias', 'transformer.layer.6.ff.layer_norm.weight', 'transformer.layer.6.ff.layer_norm.bias', 'transformer.layer.6.ff.layer_1.weight', 'transformer.layer.6.ff.layer_1.bias', 'transformer.layer.6.ff.layer_2.weight', 'transformer.layer.6.ff.layer_2.bias', 'transformer.layer.7.rel_attn.q', 'transformer.layer.7.rel_attn.k', 'transformer.layer.7.rel_attn.v', 'transformer.layer.7.rel_attn.o', 'transformer.layer.7.rel_attn.r', 'transformer.layer.7.rel_attn.r_r_bias', 'transformer.layer.7.rel_attn.r_s_bias', 'transformer.layer.7.rel_attn.r_w_bias', 'transformer.layer.7.rel_attn.seg_embed', 'transformer.layer.7.rel_attn.layer_norm.weight', 'transformer.layer.7.rel_attn.layer_norm.bias', 'transformer.layer.7.ff.layer_norm.weight', 'transformer.layer.7.ff.layer_norm.bias', 'transformer.layer.7.ff.layer_1.weight', 'transformer.layer.7.ff.layer_1.bias', 'transformer.layer.7.ff.layer_2.weight', 'transformer.layer.7.ff.layer_2.bias', 'transformer.layer.8.rel_attn.q', 'transformer.layer.8.rel_attn.k', 'transformer.layer.8.rel_attn.v', 'transformer.layer.8.rel_attn.o', 'transformer.layer.8.rel_attn.r', 'transformer.layer.8.rel_attn.r_r_bias', 'transformer.layer.8.rel_attn.r_s_bias', 'transformer.layer.8.rel_attn.r_w_bias', 'transformer.layer.8.rel_attn.seg_embed', 'transformer.layer.8.rel_attn.layer_norm.weight', 'transformer.layer.8.rel_attn.layer_norm.bias', 'transformer.layer.8.ff.layer_norm.weight', 'transformer.layer.8.ff.layer_norm.bias', 'transformer.layer.8.ff.layer_1.weight', 'transformer.layer.8.ff.layer_1.bias', 'transformer.layer.8.ff.layer_2.weight', 'transformer.layer.8.ff.layer_2.bias', 'transformer.layer.9.rel_attn.q', 'transformer.layer.9.rel_attn.k', 'transformer.layer.9.rel_attn.v', 'transformer.layer.9.rel_attn.o', 'transformer.layer.9.rel_attn.r', 'transformer.layer.9.rel_attn.r_r_bias', 'transformer.layer.9.rel_attn.r_s_bias', 'transformer.layer.9.rel_attn.r_w_bias', 'transformer.layer.9.rel_attn.seg_embed', 'transformer.layer.9.rel_attn.layer_norm.weight', 'transformer.layer.9.rel_attn.layer_norm.bias', 'transformer.layer.9.ff.layer_norm.weight', 'transformer.layer.9.ff.layer_norm.bias', 'transformer.layer.9.ff.layer_1.weight', 'transformer.layer.9.ff.layer_1.bias', 'transformer.layer.9.ff.layer_2.weight', 'transformer.layer.9.ff.layer_2.bias', 'transformer.layer.10.rel_attn.q', 'transformer.layer.10.rel_attn.k', 'transformer.layer.10.rel_attn.v', 'transformer.layer.10.rel_attn.o', 'transformer.layer.10.rel_attn.r', 'transformer.layer.10.rel_attn.r_r_bias', 'transformer.layer.10.rel_attn.r_s_bias', 'transformer.layer.10.rel_attn.r_w_bias', 'transformer.layer.10.rel_attn.seg_embed', 'transformer.layer.10.rel_attn.layer_norm.weight', 'transformer.layer.10.rel_attn.layer_norm.bias', 'transformer.layer.10.ff.layer_norm.weight', 'transformer.layer.10.ff.layer_norm.bias', 'transformer.layer.10.ff.layer_1.weight', 'transformer.layer.10.ff.layer_1.bias', 'transformer.layer.10.ff.layer_2.weight', 'transformer.layer.10.ff.layer_2.bias', 'transformer.layer.11.rel_attn.q', 'transformer.layer.11.rel_attn.k', 'transformer.layer.11.rel_attn.v', 'transformer.layer.11.rel_attn.o', 'transformer.layer.11.rel_attn.r', 'transformer.layer.11.rel_attn.r_r_bias', 'transformer.layer.11.rel_attn.r_s_bias', 'transformer.layer.11.rel_attn.r_w_bias', 'transformer.layer.11.rel_attn.seg_embed', 'transformer.layer.11.rel_attn.layer_norm.weight', 'transformer.layer.11.rel_attn.layer_norm.bias', 'transformer.layer.11.ff.layer_norm.weight', 'transformer.layer.11.ff.layer_norm.bias', 'transformer.layer.11.ff.layer_1.weight', 'transformer.layer.11.ff.layer_1.bias', 'transformer.layer.11.ff.layer_2.weight', 'transformer.layer.11.ff.layer_2.bias']\n",
      "- This IS expected if you are initializing XLNetDPCNN from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetDPCNN from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetDPCNN were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['transformer.xlnet.mask_emb', 'transformer.xlnet.word_embedding.weight', 'transformer.xlnet.layer.0.rel_attn.q', 'transformer.xlnet.layer.0.rel_attn.k', 'transformer.xlnet.layer.0.rel_attn.v', 'transformer.xlnet.layer.0.rel_attn.o', 'transformer.xlnet.layer.0.rel_attn.r', 'transformer.xlnet.layer.0.rel_attn.r_r_bias', 'transformer.xlnet.layer.0.rel_attn.r_s_bias', 'transformer.xlnet.layer.0.rel_attn.r_w_bias', 'transformer.xlnet.layer.0.rel_attn.seg_embed', 'transformer.xlnet.layer.0.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.0.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.0.ff.layer_norm.weight', 'transformer.xlnet.layer.0.ff.layer_norm.bias', 'transformer.xlnet.layer.0.ff.layer_1.weight', 'transformer.xlnet.layer.0.ff.layer_1.bias', 'transformer.xlnet.layer.0.ff.layer_2.weight', 'transformer.xlnet.layer.0.ff.layer_2.bias', 'transformer.xlnet.layer.1.rel_attn.q', 'transformer.xlnet.layer.1.rel_attn.k', 'transformer.xlnet.layer.1.rel_attn.v', 'transformer.xlnet.layer.1.rel_attn.o', 'transformer.xlnet.layer.1.rel_attn.r', 'transformer.xlnet.layer.1.rel_attn.r_r_bias', 'transformer.xlnet.layer.1.rel_attn.r_s_bias', 'transformer.xlnet.layer.1.rel_attn.r_w_bias', 'transformer.xlnet.layer.1.rel_attn.seg_embed', 'transformer.xlnet.layer.1.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.1.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.1.ff.layer_norm.weight', 'transformer.xlnet.layer.1.ff.layer_norm.bias', 'transformer.xlnet.layer.1.ff.layer_1.weight', 'transformer.xlnet.layer.1.ff.layer_1.bias', 'transformer.xlnet.layer.1.ff.layer_2.weight', 'transformer.xlnet.layer.1.ff.layer_2.bias', 'transformer.xlnet.layer.2.rel_attn.q', 'transformer.xlnet.layer.2.rel_attn.k', 'transformer.xlnet.layer.2.rel_attn.v', 'transformer.xlnet.layer.2.rel_attn.o', 'transformer.xlnet.layer.2.rel_attn.r', 'transformer.xlnet.layer.2.rel_attn.r_r_bias', 'transformer.xlnet.layer.2.rel_attn.r_s_bias', 'transformer.xlnet.layer.2.rel_attn.r_w_bias', 'transformer.xlnet.layer.2.rel_attn.seg_embed', 'transformer.xlnet.layer.2.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.2.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.2.ff.layer_norm.weight', 'transformer.xlnet.layer.2.ff.layer_norm.bias', 'transformer.xlnet.layer.2.ff.layer_1.weight', 'transformer.xlnet.layer.2.ff.layer_1.bias', 'transformer.xlnet.layer.2.ff.layer_2.weight', 'transformer.xlnet.layer.2.ff.layer_2.bias', 'transformer.xlnet.layer.3.rel_attn.q', 'transformer.xlnet.layer.3.rel_attn.k', 'transformer.xlnet.layer.3.rel_attn.v', 'transformer.xlnet.layer.3.rel_attn.o', 'transformer.xlnet.layer.3.rel_attn.r', 'transformer.xlnet.layer.3.rel_attn.r_r_bias', 'transformer.xlnet.layer.3.rel_attn.r_s_bias', 'transformer.xlnet.layer.3.rel_attn.r_w_bias', 'transformer.xlnet.layer.3.rel_attn.seg_embed', 'transformer.xlnet.layer.3.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.3.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.3.ff.layer_norm.weight', 'transformer.xlnet.layer.3.ff.layer_norm.bias', 'transformer.xlnet.layer.3.ff.layer_1.weight', 'transformer.xlnet.layer.3.ff.layer_1.bias', 'transformer.xlnet.layer.3.ff.layer_2.weight', 'transformer.xlnet.layer.3.ff.layer_2.bias', 'transformer.xlnet.layer.4.rel_attn.q', 'transformer.xlnet.layer.4.rel_attn.k', 'transformer.xlnet.layer.4.rel_attn.v', 'transformer.xlnet.layer.4.rel_attn.o', 'transformer.xlnet.layer.4.rel_attn.r', 'transformer.xlnet.layer.4.rel_attn.r_r_bias', 'transformer.xlnet.layer.4.rel_attn.r_s_bias', 'transformer.xlnet.layer.4.rel_attn.r_w_bias', 'transformer.xlnet.layer.4.rel_attn.seg_embed', 'transformer.xlnet.layer.4.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.4.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.4.ff.layer_norm.weight', 'transformer.xlnet.layer.4.ff.layer_norm.bias', 'transformer.xlnet.layer.4.ff.layer_1.weight', 'transformer.xlnet.layer.4.ff.layer_1.bias', 'transformer.xlnet.layer.4.ff.layer_2.weight', 'transformer.xlnet.layer.4.ff.layer_2.bias', 'transformer.xlnet.layer.5.rel_attn.q', 'transformer.xlnet.layer.5.rel_attn.k', 'transformer.xlnet.layer.5.rel_attn.v', 'transformer.xlnet.layer.5.rel_attn.o', 'transformer.xlnet.layer.5.rel_attn.r', 'transformer.xlnet.layer.5.rel_attn.r_r_bias', 'transformer.xlnet.layer.5.rel_attn.r_s_bias', 'transformer.xlnet.layer.5.rel_attn.r_w_bias', 'transformer.xlnet.layer.5.rel_attn.seg_embed', 'transformer.xlnet.layer.5.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.5.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.5.ff.layer_norm.weight', 'transformer.xlnet.layer.5.ff.layer_norm.bias', 'transformer.xlnet.layer.5.ff.layer_1.weight', 'transformer.xlnet.layer.5.ff.layer_1.bias', 'transformer.xlnet.layer.5.ff.layer_2.weight', 'transformer.xlnet.layer.5.ff.layer_2.bias', 'transformer.xlnet.layer.6.rel_attn.q', 'transformer.xlnet.layer.6.rel_attn.k', 'transformer.xlnet.layer.6.rel_attn.v', 'transformer.xlnet.layer.6.rel_attn.o', 'transformer.xlnet.layer.6.rel_attn.r', 'transformer.xlnet.layer.6.rel_attn.r_r_bias', 'transformer.xlnet.layer.6.rel_attn.r_s_bias', 'transformer.xlnet.layer.6.rel_attn.r_w_bias', 'transformer.xlnet.layer.6.rel_attn.seg_embed', 'transformer.xlnet.layer.6.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.6.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.6.ff.layer_norm.weight', 'transformer.xlnet.layer.6.ff.layer_norm.bias', 'transformer.xlnet.layer.6.ff.layer_1.weight', 'transformer.xlnet.layer.6.ff.layer_1.bias', 'transformer.xlnet.layer.6.ff.layer_2.weight', 'transformer.xlnet.layer.6.ff.layer_2.bias', 'transformer.xlnet.layer.7.rel_attn.q', 'transformer.xlnet.layer.7.rel_attn.k', 'transformer.xlnet.layer.7.rel_attn.v', 'transformer.xlnet.layer.7.rel_attn.o', 'transformer.xlnet.layer.7.rel_attn.r', 'transformer.xlnet.layer.7.rel_attn.r_r_bias', 'transformer.xlnet.layer.7.rel_attn.r_s_bias', 'transformer.xlnet.layer.7.rel_attn.r_w_bias', 'transformer.xlnet.layer.7.rel_attn.seg_embed', 'transformer.xlnet.layer.7.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.7.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.7.ff.layer_norm.weight', 'transformer.xlnet.layer.7.ff.layer_norm.bias', 'transformer.xlnet.layer.7.ff.layer_1.weight', 'transformer.xlnet.layer.7.ff.layer_1.bias', 'transformer.xlnet.layer.7.ff.layer_2.weight', 'transformer.xlnet.layer.7.ff.layer_2.bias', 'transformer.xlnet.layer.8.rel_attn.q', 'transformer.xlnet.layer.8.rel_attn.k', 'transformer.xlnet.layer.8.rel_attn.v', 'transformer.xlnet.layer.8.rel_attn.o', 'transformer.xlnet.layer.8.rel_attn.r', 'transformer.xlnet.layer.8.rel_attn.r_r_bias', 'transformer.xlnet.layer.8.rel_attn.r_s_bias', 'transformer.xlnet.layer.8.rel_attn.r_w_bias', 'transformer.xlnet.layer.8.rel_attn.seg_embed', 'transformer.xlnet.layer.8.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.8.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.8.ff.layer_norm.weight', 'transformer.xlnet.layer.8.ff.layer_norm.bias', 'transformer.xlnet.layer.8.ff.layer_1.weight', 'transformer.xlnet.layer.8.ff.layer_1.bias', 'transformer.xlnet.layer.8.ff.layer_2.weight', 'transformer.xlnet.layer.8.ff.layer_2.bias', 'transformer.xlnet.layer.9.rel_attn.q', 'transformer.xlnet.layer.9.rel_attn.k', 'transformer.xlnet.layer.9.rel_attn.v', 'transformer.xlnet.layer.9.rel_attn.o', 'transformer.xlnet.layer.9.rel_attn.r', 'transformer.xlnet.layer.9.rel_attn.r_r_bias', 'transformer.xlnet.layer.9.rel_attn.r_s_bias', 'transformer.xlnet.layer.9.rel_attn.r_w_bias', 'transformer.xlnet.layer.9.rel_attn.seg_embed', 'transformer.xlnet.layer.9.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.9.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.9.ff.layer_norm.weight', 'transformer.xlnet.layer.9.ff.layer_norm.bias', 'transformer.xlnet.layer.9.ff.layer_1.weight', 'transformer.xlnet.layer.9.ff.layer_1.bias', 'transformer.xlnet.layer.9.ff.layer_2.weight', 'transformer.xlnet.layer.9.ff.layer_2.bias', 'transformer.xlnet.layer.10.rel_attn.q', 'transformer.xlnet.layer.10.rel_attn.k', 'transformer.xlnet.layer.10.rel_attn.v', 'transformer.xlnet.layer.10.rel_attn.o', 'transformer.xlnet.layer.10.rel_attn.r', 'transformer.xlnet.layer.10.rel_attn.r_r_bias', 'transformer.xlnet.layer.10.rel_attn.r_s_bias', 'transformer.xlnet.layer.10.rel_attn.r_w_bias', 'transformer.xlnet.layer.10.rel_attn.seg_embed', 'transformer.xlnet.layer.10.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.10.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.10.ff.layer_norm.weight', 'transformer.xlnet.layer.10.ff.layer_norm.bias', 'transformer.xlnet.layer.10.ff.layer_1.weight', 'transformer.xlnet.layer.10.ff.layer_1.bias', 'transformer.xlnet.layer.10.ff.layer_2.weight', 'transformer.xlnet.layer.10.ff.layer_2.bias', 'transformer.xlnet.layer.11.rel_attn.q', 'transformer.xlnet.layer.11.rel_attn.k', 'transformer.xlnet.layer.11.rel_attn.v', 'transformer.xlnet.layer.11.rel_attn.o', 'transformer.xlnet.layer.11.rel_attn.r', 'transformer.xlnet.layer.11.rel_attn.r_r_bias', 'transformer.xlnet.layer.11.rel_attn.r_s_bias', 'transformer.xlnet.layer.11.rel_attn.r_w_bias', 'transformer.xlnet.layer.11.rel_attn.seg_embed', 'transformer.xlnet.layer.11.rel_attn.layer_norm.weight', 'transformer.xlnet.layer.11.rel_attn.layer_norm.bias', 'transformer.xlnet.layer.11.ff.layer_norm.weight', 'transformer.xlnet.layer.11.ff.layer_norm.bias', 'transformer.xlnet.layer.11.ff.layer_1.weight', 'transformer.xlnet.layer.11.ff.layer_1.bias', 'transformer.xlnet.layer.11.ff.layer_2.weight', 'transformer.xlnet.layer.11.ff.layer_2.bias', 'transformer.dpcnn.conv_embedding.weight', 'transformer.dpcnn.conv_embedding.bias', 'transformer.dpcnn.conv2.weight', 'transformer.dpcnn.conv2.bias', 'transformer.dpcnn.last_linear.weight', 'transformer.dpcnn.last_linear.bias', 'transformer.classifier.weight', 'transformer.classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLNetDPCNN(\n",
       "  (xlnet): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (dpcnn): DPCNN(\n",
       "    (conv_embedding): Conv2d(1, 250, kernel_size=(3, 768), stride=(1, 1))\n",
       "    (conv2): Conv2d(250, 250, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (pooling): MaxPool2d(kernel_size=(3, 1), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (padding_conv): ZeroPad2d(padding=(0, 0, 1, 1), value=0.0)\n",
       "    (padding_pool): ZeroPad2d(padding=(0, 0, 0, 1), value=0.0)\n",
       "    (activation_function): ReLU()\n",
       "    (last_linear): Linear(in_features=250, out_features=12, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=12, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLNetDPCNN.from_pretrained(\n",
    "    'xlnet-base-cased', num_labels=len(label_columns))\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3gUYvgvzwTn"
   },
   "outputs": [],
   "source": [
    "# Set epoch and grad max num\n",
    "epochs = 2\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 984,
     "status": "ok",
     "timestamp": 1574044432183,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "74tTvA4Iz1ED",
    "outputId": "549be8f8-ede6-4642-bbae-ffd8cf270323"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    betas: (0.9, 0.999)\n",
       "    correct_bias: True\n",
       "    eps: 1e-06\n",
       "    lr: 3e-05\n",
       "    weight_decay: 0.0\n",
       "    weight_decay_rate: 0.01\n",
       "\n",
       "Parameter Group 1\n",
       "    betas: (0.9, 0.999)\n",
       "    correct_bias: True\n",
       "    eps: 1e-06\n",
       "    lr: 3e-05\n",
       "    weight_decay: 0.0\n",
       "    weight_decay_rate: 0.0\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine tune model all layer parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "      'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "      'weight_decay_rate': 0.0}\n",
    "]\n",
    "    \n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FoP-2uA1vix"
   },
   "source": [
    "#### Fine-tuning the XLNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCvUimI91zEF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetDPCNN(\n",
       "  (xlnet): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (dpcnn): DPCNN(\n",
       "    (conv_embedding): Conv2d(1, 250, kernel_size=(3, 768), stride=(1, 1))\n",
       "    (conv2): Conv2d(250, 250, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (pooling): MaxPool2d(kernel_size=(3, 1), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (padding_conv): ZeroPad2d(padding=(0, 0, 1, 1), value=0.0)\n",
       "    (padding_pool): ZeroPad2d(padding=(0, 0, 0, 1), value=0.0)\n",
       "    (activation_function): ReLU()\n",
       "    (last_linear): Linear(in_features=250, out_features=12, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=12, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 154531,
     "status": "ok",
     "timestamp": 1574043151918,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "MqDr9Vl111Hi",
    "outputId": "3b871d45-9d7d-4b83-8e84-ad2a55e5b53c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 25716\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 1/2 [24:39<24:39, 1479.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2642765222673408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2/2 [49:31<00:00, 1485.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1734589641481789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(tt_train_input_ids)))\n",
    "print(\"  Batch size = %d\"%(batch_num))\n",
    "for _ in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_segs, b_labels = batch\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(\n",
    "            input_ids = b_input_ids, \n",
    "            token_type_ids = b_segs, \n",
    "            input_mask = b_input_mask,\n",
    "            labels= b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZfxzT3cChl5"
   },
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1702,
     "status": "ok",
     "timestamp": 1574043632315,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "irtWQYI9Ckuj",
    "outputId": "4c268caf-9dec-48be-9f9a-7a57a37016d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../xlnet/spiece.model',)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_name = MODEL_PATH + \"/pytorch_model.bin\"\n",
    "model_config_save_name = MODEL_PATH + \"/config.json\"\n",
    "model_report_save_name = MODEL_PATH + \"/eval_results.txt\"\n",
    "\n",
    "torch.save(model.state_dict(), model_save_name)\n",
    "model.config.to_json_file(model_config_save_name)\n",
    "tokenizer.save_vocabulary(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GfezBjSkDxxv"
   },
   "source": [
    "#### Loading finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2454,
     "status": "ok",
     "timestamp": 1574043638363,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "vzHLlBrcDvnF",
    "outputId": "8d2855aa-7707-4556-8fcc-7b1d6ebc9b67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLNetDPCNN(\n",
       "  (xlnet): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (dpcnn): DPCNN(\n",
       "    (conv_embedding): Conv2d(1, 250, kernel_size=(3, 768), stride=(1, 1))\n",
       "    (conv2): Conv2d(250, 250, kernel_size=(3, 1), stride=(1, 1))\n",
       "    (pooling): MaxPool2d(kernel_size=(3, 1), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (padding_conv): ZeroPad2d(padding=(0, 0, 1, 1), value=0.0)\n",
       "    (padding_pool): ZeroPad2d(padding=(0, 0, 0, 1), value=0.0)\n",
       "    (activation_function): ReLU()\n",
       "    (last_linear): Linear(in_features=250, out_features=12, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=12, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLNetDPCNN.from_pretrained(\n",
    "    MODEL_PATH, num_labels=len(label_columns))\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3iXL7gbCAIt"
   },
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FymaCDX6-5DW"
   },
   "outputs": [],
   "source": [
    "root_folder = '../xlnet'\n",
    "\n",
    "def load_model(folder=MODEL_PATH):\n",
    "    model = XLNetDPCNN.from_pretrained(MODEL_PATH, num_labels=len(label_columns))\n",
    "    \n",
    "    if n_gpu != 0:\n",
    "        model.cuda()\n",
    "      \n",
    "    return model\n",
    "\n",
    "def save_object(filename, obj, folder=root_folder):\n",
    "    obj_file = os.path.join(folder, filename)\n",
    "\n",
    "    if os.path.exists(obj_file):\n",
    "        os.remove(obj_file)\n",
    "\n",
    "    with open(obj_file, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "\n",
    "def load_object(filename, folder=root_folder):\n",
    "    object_file = os.path.join(folder, filename)\n",
    "\n",
    "    with open(object_file, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def calculate_confusion_matrix(data_loader, model, batch_size):\n",
    "    model.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            # add batch to gpu\n",
    "            if n_gpu != 0:\n",
    "                batch = tuple(t.to('cuda') for t in batch)\n",
    "            \n",
    "            logger.info(f'Step {step}')\n",
    "            #import pdb; pdb.set_trace()\n",
    "            b_input_ids, b_input_masks, b_segment_ids, b_labels = batch\n",
    "        \n",
    "            # forward pass\n",
    "            #raw_outputs = model(b_input_ids, b_segment_ids, b_input_masks)\n",
    "            raw_outputs = model(input_ids = b_input_ids, token_type_ids = b_segment_ids, input_mask = b_input_masks)\n",
    "            predictions.append((torch.sigmoid(raw_outputs[0]) > .5).cpu().numpy().astype(int))\n",
    "            true_labels.append(b_labels.cpu().numpy().astype(int))\n",
    "\n",
    "        true_labels = np.vstack(true_labels)[0:(batch_size * len(predictions)),]\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        return (multilabel_confusion_matrix(true_labels, predictions),\n",
    "                true_labels,\n",
    "                predictions)\n",
    "\n",
    "def calculate_predictions(data_loader, model, batch_size):\n",
    "    if n_gpu != 0:\n",
    "        model.cuda()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()\n",
    "    result = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            logger.info(f'Step {step}')\n",
    "            \n",
    "            # add batch to gpu\n",
    "            if n_gpu != 0:\n",
    "                batch = tuple(t.to('cuda') for t in batch)\n",
    "                \n",
    "            b_input_ids, b_input_masks, b_segment_ids, b_labels = batch\n",
    "        \n",
    "            # forward pass\n",
    "            raw_outputs = model(b_input_ids, b_segment_ids, b_input_masks)\n",
    "            predictions = torch.sigmoid(raw_outputs[2]).cpu()\n",
    "            predictions = predictions.numpy()\n",
    "            result = predictions if result is None else np.append(result, predictions, axis=0)\n",
    "            \n",
    "        return result\n",
    "\n",
    "def get_label_columns(data_frame):\n",
    "    skip_columns =  { 'Id', 'Text', 'input_ids', 'attention_masks', 'segment_masks' }\n",
    "    return [c for c in data_frame.columns if c not in skip_columns and '_Pred' not in c]\n",
    "\n",
    "def add_predictions(data_frame, features, model, batch_size):\n",
    "    logits = calculate_predictions(features, model, batch_size)\n",
    "    predictions = (logits > .5).astype(int)\n",
    "    \n",
    "    for i, column_name in enumerate(get_label_columns(data_frame)):\n",
    "        data_frame[f'{column_name}_Pred'] = predictions[:, i]\n",
    "\n",
    "def add_predictions_to_dataframe(data_frame, predictions):\n",
    "    for i, column_name in enumerate(get_label_columns(data_frame)):\n",
    "        data_frame[f'{column_name}_Pred'] = predictions[:, i]\n",
    "        \n",
    "class ModelResult:\n",
    "    def __init__(self, data_frame, features, model, batch_size):\n",
    "        confusion_matrix, labels, predictions = calculate_confusion_matrix(features, model, batch_size)\n",
    "        self.confusion_matrix = confusion_matrix\n",
    "        self.labels = labels\n",
    "        self.predictions = predictions\n",
    "        self.label_columns = get_label_columns(data_frame)\n",
    "        \n",
    "    def save(self, filename):\n",
    "        save_object(filename, self)\n",
    "        \n",
    "    def get_classification_report(self, output_dictionary):\n",
    "        return classification_report(self.labels, self.predictions, \n",
    "                                     target_names=self.label_columns, \n",
    "                                     output_dict=output_dictionary)\n",
    "        \n",
    "    def get_results_dataframe(self):\n",
    "        report_dict = self.get_classification_report(True)\n",
    "        label_columns = list(self.label_columns)\n",
    "        label_columns.append('weighted avg')\n",
    "        classification_results = pd.DataFrame({'Label':label_columns})\n",
    "        classification_results['Precision'] = [report_dict[c]['precision'] for c in label_columns]\n",
    "        classification_results['Recall'] = [report_dict[c]['recall'] for c in label_columns]\n",
    "        classification_results['F1-Score'] = [report_dict[c]['f1-score'] for c in label_columns]\n",
    "        classification_results['Support'] = [report_dict[c]['support'] for c in label_columns]\n",
    "        classification_results.loc[classification_results['Label'] == 'weighted avg', 'Label'] = 'Weighted Average'\n",
    "        \n",
    "        return classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = ModelResult(test_df, valid_dataloader, model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.save('xlnet_dpcnn_result_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = load_object('xlnet_dpcnn_result_test.pkl')\n",
    "model_results.label_columns = get_label_columns(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-febcba5a553d>:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_frame[f'{column_name}_Pred'] = predictions[:, i]\n"
     ]
    }
   ],
   "source": [
    "add_predictions_to_dataframe(test_df, model_results.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df[get_label_columns(test_df)].sum(1).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World</td>\n",
       "      <td>0.760479</td>\n",
       "      <td>0.573363</td>\n",
       "      <td>0.653797</td>\n",
       "      <td>886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Washington</td>\n",
       "      <td>0.866883</td>\n",
       "      <td>0.359838</td>\n",
       "      <td>0.508571</td>\n",
       "      <td>742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New_York_and_Region</td>\n",
       "      <td>0.859075</td>\n",
       "      <td>0.660646</td>\n",
       "      <td>0.746906</td>\n",
       "      <td>1827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Front_Page</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.188811</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.766007</td>\n",
       "      <td>0.821986</td>\n",
       "      <td>859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>US</td>\n",
       "      <td>0.736527</td>\n",
       "      <td>0.625954</td>\n",
       "      <td>0.676754</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sports</td>\n",
       "      <td>0.916480</td>\n",
       "      <td>0.965171</td>\n",
       "      <td>0.940196</td>\n",
       "      <td>1694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Obituaries</td>\n",
       "      <td>0.973978</td>\n",
       "      <td>0.966790</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Health</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Education</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Science</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Technology</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.167832</td>\n",
       "      <td>0.238806</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Weighted Average</td>\n",
       "      <td>0.832797</td>\n",
       "      <td>0.632666</td>\n",
       "      <td>0.686060</td>\n",
       "      <td>9245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Label  Precision    Recall  F1-Score  Support\n",
       "0                 World   0.760479  0.573363  0.653797      886\n",
       "1            Washington   0.866883  0.359838  0.508571      742\n",
       "2   New_York_and_Region   0.859075  0.660646  0.746906     1827\n",
       "3            Front_Page   0.818182  0.188811  0.306818      286\n",
       "4              Business   0.886792  0.766007  0.821986      859\n",
       "5                    US   0.736527  0.625954  0.676754     1965\n",
       "6                Sports   0.916480  0.965171  0.940196     1694\n",
       "7            Obituaries   0.973978  0.966790  0.970370      271\n",
       "8                Health   1.000000  0.006623  0.013158      302\n",
       "9             Education   0.000000  0.000000  0.000000       78\n",
       "10              Science   1.000000  0.010417  0.020619      192\n",
       "11           Technology   0.413793  0.167832  0.238806      143\n",
       "12     Weighted Average   0.832797  0.632666  0.686060     9245"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results.get_results_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Label'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_results.get_results_dataframe().plot.barh(x='Label', y='Precision')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame({'x':((model_results.labels == model_results.predictions).astype(int).sum(1))}).plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUewusx3EKeY"
   },
   "outputs": [],
   "source": [
    "# Softtmax accuracy\n",
    "#def accuracy(out, labels):\n",
    "#    outputs = np.argmax(out, axis=1)\n",
    "#    return np.sum(outputs == labels)\n",
    "\n",
    "# Multilabel accuracy\n",
    "def accuracy(y_pred, y_true, thresh=0.5):\n",
    "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "    return np.mean(((y_pred > thresh) == y_true), axis=1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3415,
     "status": "ok",
     "timestamp": 1574043652581,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "Zl1XtVnFiWL0",
    "outputId": "3b644ebd-e939-4055-f6b4-1923dd32b173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num examples =6430\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "all_logits = None\n",
    "all_labels = None\n",
    "\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num examples ={}\".format(len(tt_test_input_ids)))\n",
    "print(\"  Batch size = {}\".format(batch_num))\n",
    "\n",
    "for step, batch in enumerate(valid_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_segs,b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids = b_input_ids,\n",
    "            token_type_ids = b_segs, \n",
    "            input_mask = b_input_mask,\n",
    "            labels = b_labels)\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "    \n",
    "    # Get textclassification predict result\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "    \n",
    "    # Save predict and real label reuslt for analyze\n",
    "    if all_logits is None:\n",
    "        all_logits = logits\n",
    "    else:\n",
    "        all_logits = np.concatenate((all_logits, logits), axis=0)\n",
    "        \n",
    "    if all_labels is None:\n",
    "        all_labels = label_ids\n",
    "    else:    \n",
    "        all_labels = np.concatenate((all_labels, label_ids), axis=0)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "   \n",
    "    nb_eval_steps += 1\n",
    "\n",
    "\n",
    "    \n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / len(tt_test_input_ids)\n",
    "\n",
    "#     ROC-AUC calcualation\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(len(label_columns)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(all_labels[:, i], all_logits[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels.ravel(), all_logits.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,\n",
    "          'roc_auc': roc_auc}\n",
    "\n",
    "with open(model_report_save_name, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 574,
     "status": "ok",
     "timestamp": 1574043668109,
     "user": {
      "displayName": "Carlos Sancini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBK_a0VL329YfM0B72acZH97KLpfXxCOtGUCi9s=s64",
      "userId": "02726390835148099400"
     },
     "user_tz": 180
    },
    "id": "IRbZ-5MCk8uV",
    "outputId": "830db9e0-742d-41fa-cef4-4142d9afc7e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.16361501278451357,\n",
       " 'eval_accuracy': 0.9390876101607047,\n",
       " 'roc_auc': {0: 0.9109412815676923,\n",
       "  1: 0.9318522041011293,\n",
       "  2: 0.9323621193241456,\n",
       "  3: 0.8127680425043706,\n",
       "  4: 0.9635483437533761,\n",
       "  5: 0.867810479585353,\n",
       "  6: 0.9929611715833306,\n",
       "  7: 0.9978185704896504,\n",
       "  8: 0.8426287759583628,\n",
       "  9: 0.7533565039075115,\n",
       "  10: 0.8666364419685795,\n",
       "  11: 0.8583924426138518,\n",
       "  'micro': 0.9458338774643821}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "XLNet Multilabel-DPCNN NYT.ipynb",
   "provenance": [
    {
     "file_id": "1Wc5wSxt4Ity1kXyajryJcSDrhDBQsHtW",
     "timestamp": 1574034708446
    },
    {
     "file_id": "1q3FAA-0F0G2e6qKhQ_navKJ3vcc2A9W1",
     "timestamp": 1573322451215
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
